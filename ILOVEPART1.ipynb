{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from zipfile import ZipFile\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path where your ZIP files are located locally\n",
    "# Use a raw string for the path\n",
    "#directory_path = \"/Users/biancabostrom/Documents/ADA/Wedge Project/WedgeZipOfZips_Big\"\n",
    "directory_path = r'C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big'\n",
    "output_folder = 'extracted_zips_big'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over all the files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Create a folder for each ZIP file\n",
    "        folder_name = os.path.splitext(filename)[0]\n",
    "        extract_path = os.path.join(output_folder, folder_name)\n",
    "\n",
    "        # Print the file path for debugging\n",
    "        print(f\"Attempting to extract: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all the contents into the directory\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "print(\"All files extracted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning files: headers, delimeters, nulls and quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each folder to check and add headers if different\n",
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "\n",
    "for this_folder in os.listdir(output_folder):\n",
    "    folder_path = os.path.join(output_folder, this_folder)\n",
    "\n",
    "    # Check if the entry is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            input_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Read the content of the file\n",
    "            with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n",
    "                content = input_file.read()\n",
    "\n",
    "            # Check if headers are present\n",
    "            has_headers = all(header in content for header in correct_headers)\n",
    "\n",
    "            if not has_headers:\n",
    "                # Headers are missing, add them\n",
    "                content = \",\".join(correct_headers) + \"\\n\" + content\n",
    "\n",
    "                # Write the modified content back to the file\n",
    "                with open(input_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "                    output_file.write(content)\n",
    "\n",
    "                print(f\"Headers added to file {file_name} in folder {this_folder}.\")\n",
    "\n",
    "            print(f\"File {file_name} in folder {this_folder} is good.\")\n",
    "\n",
    "print(\"Done checking and adding headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for delimiters\n",
    "delimiters = dict()\n",
    "\n",
    "# Start by reading in all the files again.\n",
    "for this_folder in os.listdir(output_folder):\n",
    "    folder_path = os.path.join(output_folder, this_folder)\n",
    "\n",
    "    # Check if the entry is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.csv'):  # Check if the file is a CSV file\n",
    "                input_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                # Read the content of the file\n",
    "                with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n",
    "                    # Read only the first line to determine the delimiter\n",
    "                    first_line = input_file.readline()\n",
    "\n",
    "                    dialect = csv.Sniffer().sniff(sample=first_line, delimiters=[\",\", \";\", \"\\t\"])\n",
    "                    delimiters[file_name] = dialect.delimiter\n",
    "\n",
    "                    print(\" \".join([\"It looks like\",\n",
    "                                    file_name,\n",
    "                                    \"in folder\",\n",
    "                                    this_folder,\n",
    "                                    \"has delimiter\",\n",
    "                                    dialect.delimiter,\n",
    "                                    \".\"]))\n",
    "\n",
    "                # You can add the rest of your processing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each file and replace the delimters to \",\" in the files that don't\n",
    "\n",
    "# Clean the files\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'a') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file_path = os.path.join(zip_folder, this_zf, file_name)\n",
    "            \n",
    "            # Read the content of the file\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                content = input_file.read().decode(\"utf-8\")\n",
    "            \n",
    "            # Check if the delimiter is not a comma\n",
    "            if delimiters[file_name] != \",\":\n",
    "                # Replace the delimiter with a comma\n",
    "                content = content.replace(delimiters[file_name], \",\")\n",
    "\n",
    "                # Write the modified content back to the file\n",
    "                zf.writestr(file_name, content.encode(\"utf-8\"))\n",
    "\n",
    "                print(f\"File {file_name} has been cleaned.\")\n",
    "\n",
    "print(\"Done cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace empty values with \"null\"\n",
    "\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'a') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file_path = os.path.join(zip_folder, this_zf, file_name)\n",
    "\n",
    "            # Read the content of the file\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                content = input_file.read().decode(\"utf-8\")\n",
    "\n",
    "            # Identify and replace null values (assuming nulls are represented as an empty string \"\")\n",
    "            content = content.replace('\"\"', 'null')\n",
    "\n",
    "            # Write the modified content back to the file\n",
    "            zf.writestr(file_name, content.encode(\"utf-8\"))\n",
    "\n",
    "            print(f\"Null values handled in file {file_name}.\")\n",
    "\n",
    "print(\"Done checking and handling null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_path = \"/Users/biancabostrom/Documents/ADA/Wedge\\ Project/wedge-404400-cb3a632effa5.json\"\n",
    "service_file = 'wedge-404400-cb3a632effa5.json' \n",
    "gbq_proj_id = \"wedge-404400\" \n",
    "gbq_dataset_id = \"wedge_data\"\n",
    "\n",
    "\n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\"/Users/biancabostrom/Documents/ADA/Wedge Project/wedge-404400-cb3a632effa5.json\")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the Pandas function to_gbq to upload your data to GBQ.\n",
    "\n",
    "#Iterate through each CSV file in the folder\n",
    "\n",
    "for file_name in os.listdir(zip_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(zip_folder, file_name)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Define the BigQuery table ID in the format 'project.dataset.table'\n",
    "        table_id = f'your_project.your_dataset.{file_name.replace(\".csv\", \"\")}'\n",
    "\n",
    "        # Upload the DataFrame to BigQuery using to_gbq\n",
    "        df.to_gbq(destination_table=table_id, project_id='your_project', if_exists='replace')\n",
    "\n",
    "print(\"Upload complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
