{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from zipfile import ZipFile\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path where your ZIP files are located locally\n",
    "# Use a raw string for the path\n",
    "#directory_path = \"/Users/biancabostrom/Documents/ADA/Wedge Project/WedgeZipOfZips_Big\"\n",
    "directory_path = r'C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big'\n",
    "output_folder = 'data\\\\extracted_zips_big'\n",
    "clean_output_folder = 'data\\\\clean_csvs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract CSV's from zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over all the files in the directory\n",
    "for idx, filename in enumerate(os.listdir(directory_path)) : # JC: I added the enumerate so I can test on small samples.\n",
    "    if filename.endswith('.zip'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Print the file path for debugging\n",
    "        print(f\"Attempting to extract: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all the contents into the directory\n",
    "                zip_ref.extractall(output_folder) # JC: so I changed this\n",
    "                print(f\"Extracted {filename} to {output_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "        #if idx > 10:\n",
    "            #break\n",
    "print(\"All files extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn messy CSV to clean CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning files: headers, delimeters, nulls and quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(clean_output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "# loop though all files in the directory\n",
    "# JC: you did this in a bizarre way. Compare to this.\n",
    "\n",
    "extracted_files = os.listdir(output_folder)\n",
    "\n",
    "for file in extracted_files : \n",
    "    # Now we pick up with yours. \n",
    "    if file.endswith('.csv'):\n",
    "        with open(output_folder + \"/\" + file,'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "\n",
    "        print(first_line)\n",
    "\n",
    "        # Handle different delimiters\n",
    "        if \"datetime\" in first_line:\n",
    "            # Check for comma as delimiter\n",
    "            if \",\" in first_line:\n",
    "                \n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n",
    "        \n",
    "            elif \";\" in first_line:\n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n",
    "            else:\n",
    "                print(f\"Neither , or ; in {file}\")\n",
    "        else:\n",
    "            # Add headers to the file and then read it\n",
    "            if \",\" in first_line:\n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n",
    "        \n",
    "            elif \";\" in first_line:\n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\", header=None, names=correct_headers)\n",
    "            else:\n",
    "                print(f\"Neither , or ; in {file}\")\n",
    "\n",
    "##### Testing this to remove uploading error\n",
    "        #dtypes = {\"memType\": str}\n",
    "        #messy_data = messy_data.astype(dtypes)\n",
    "\n",
    "        #messy_data.to_csv(os.path.join(clean_output_folder, file.replace(\"csv\", \"txt\")), sep=\"\\t\", index=False)\n",
    "\n",
    "##### Testing above to remove uploading error\n",
    "\n",
    "        messy_data.to_csv(clean_output_folder + \"/\" + file.replace(\"csv\", \"txt\"), sep=\"\\t\", index = False)\n",
    "\n",
    "        ### Work on null NULL \\\\N - replace with \"\" (this empty string)\n",
    "        messy_data.replace([\"NULL\", \"\\\\N\"], \"\", inplace=True)  ## Chat said to put this above the above line.\n",
    "        ### some files are , delimited and some product desc have , in them. PD might handle. \n",
    "\n",
    "        #if '\"' in first_line : \n",
    "            #with open(output_folder + \"/\" + file,'r') as f:\n",
    "                #content = f.read()\n",
    "                #print(content[:1000])\n",
    "                #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bianca's info\n",
    "#service_path = \"/Users/biancabostrom/Documents/ADA/Wedge\\ Project/wedge-404400-cb3a632effa5.json\"\n",
    "#service_file = 'wedge-404400-cb3a632effa5.json' \n",
    "#gbq_proj_id = \"wedge-404400\" \n",
    "#gbq_dataset_id = \"wedge_data\"\n",
    "#credentials = service_account.Credentials.from_service_account_file(\"/Users/biancabostrom/Documents/ADA/Wedge Project/wedge-404400-cb3a632effa5.json\")\n",
    "\n",
    "# Spencer's info\n",
    "service_path = r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\"\n",
    "service_file = 'leafy-sunrise-403222-f51fcd80b921.json' # change this to your authentication information  \n",
    "gbq_proj_id = \"leafy-sunrise-403222\" # change this to your project. \n",
    "gbq_dataset_id = \"wedge_data\"\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path)\n",
    "\n",
    "private_key = service_path + service_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# John - should I use something like this to replace some of the schema definitions?\n",
    "                #df = df.clean_names()\n",
    "                #df['datetime'] = pd.to_datetime(df.datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "                #df['department'] = df['department'].astype(\"str\")\n",
    "                #df.department = df.department.fillna('')\n",
    "\n",
    "\n",
    "schema = [\n",
    "    {\"name\": \"datetime\", \"type\": \"TIMESTAMP\"},     # 1\n",
    "    {\"name\": \"register_no\", \"type\": \"FLOAT\"},      # 2\n",
    "    {\"name\": \"emp_no\", \"type\": \"FLOAT\"},           # 3\n",
    "    {\"name\": \"trans_no\", \"type\": \"FLOAT\"},         # 4\n",
    "    {\"name\": \"upc\", \"type\": \"STRING\"},             # 5\n",
    "    {\"name\": \"description\", \"type\": \"STRING\"},     # 6\n",
    "    {\"name\": \"trans_type\", \"type\": \"STRING\"},      # 7\n",
    "    {\"name\": \"trans_subtype\", \"type\": \"STRING\"},   # 8\n",
    "    {\"name\": \"trans_status\", \"type\": \"STRING\"},    # 9\n",
    "    {\"name\": \"department\", \"type\": \"FLOAT\"},       # 10\n",
    "    {\"name\": \"quantity\", \"type\": \"FLOAT\"},         # 11\n",
    "    {\"name\": \"Scale\", \"type\": \"FLOAT\"},            # 12\n",
    "    {\"name\": \"cost\", \"type\": \"FLOAT\"},             # 13\n",
    "    {\"name\": \"unitPrice\", \"type\": \"FLOAT\"},        # 14\n",
    "    {\"name\": \"total\", \"type\": \"FLOAT\"},            # 15\n",
    "    {\"name\": \"regPrice\", \"type\": \"FLOAT\"},         # 16\n",
    "    {\"name\": \"altPrice\", \"type\": \"FLOAT\"},         # 17\n",
    "    {\"name\": \"tax\", \"type\": \"FLOAT\"},              # 18\n",
    "    {\"name\": \"taxexempt\", \"type\": \"FLOAT\"},        # 19\n",
    "    {\"name\": \"foodstamp\", \"type\": \"FLOAT\"},        # 20\n",
    "    {\"name\": \"wicable\", \"type\": \"FLOAT\"},          # 21\n",
    "    {\"name\": \"discount\", \"type\": \"FLOAT\"},         # 22\n",
    "    {\"name\": \"memDiscount\", \"type\": \"FLOAT\"},      # 23\n",
    "    {\"name\": \"discountable\", \"type\": \"FLOAT\"},     # 24\n",
    "    {\"name\": \"discounttype\", \"type\": \"FLOAT\"},     # 25\n",
    "    {\"name\": \"voided\", \"type\": \"FLOAT\"},           # 26\n",
    "    {\"name\": \"percentDiscount\", \"type\": \"FLOAT\"},  # 27\n",
    "    {\"name\": \"ItemQtty\", \"type\": \"FLOAT\"},         # 28\n",
    "    {\"name\": \"volDiscType\", \"type\": \"FLOAT\"},      # 29\n",
    "    {\"name\": \"volume\", \"type\": \"FLOAT\"},           # 30\n",
    "    {\"name\": \"VolSpecial\", \"type\": \"FLOAT\"},       # 31\n",
    "    {\"name\": \"mixMatch\", \"type\": \"FLOAT\"},         # 32\n",
    "    {\"name\": \"matched\", \"type\": \"FLOAT\"},          # 33\n",
    "    {\"name\": \"memType\", \"type\": \"BOOLEAN\"},        # 34 changing this from BOOLEAN to STRING to test in GBQ uploading\n",
    "    {\"name\": \"staff\", \"type\": \"BOOLEAN\"},          # 35\n",
    "    {\"name\": \"numflag\", \"type\": \"FLOAT\"},          # 36\n",
    "    {\"name\": \"itemstatus\", \"type\": \"FLOAT\"},       # 37\n",
    "    {\"name\": \"tenderstatus\", \"type\": \"FLOAT\"},     # 38\n",
    "    {\"name\": \"charflag\", \"type\": \"STRING\"},        # 39\n",
    "    {\"name\": \"varflag\", \"type\": \"FLOAT\"},          # 40\n",
    "    {\"name\": \"batchHeaderID\", \"type\": \"BOOLEAN\"},  # 41\n",
    "    {\"name\": \"local\", \"type\": \"FLOAT\"},            # 42\n",
    "    {\"name\": \"organic\", \"type\": \"FLOAT\"},          # 43\n",
    "    {\"name\": \"display\", \"type\": \"BOOLEAN\"},        # 44\n",
    "    {\"name\": \"receipt\", \"type\": \"FLOAT\"},          # 45\n",
    "    {\"name\": \"card_no\", \"type\": \"FLOAT\"},          # 46\n",
    "    {\"name\": \"store\", \"type\": \"FLOAT\"},            # 47\n",
    "    {\"name\": \"branch\", \"type\": \"FLOAT\"},           # 48\n",
    "    {\"name\": \"match_id\", \"type\": \"FLOAT\"},         # 49\n",
    "    {\"name\": \"trans_id\", \"type\": \"FLOAT\"}          # 50\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(cleaned_data.info())\n",
    "print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(cleaned_data.isnull().sum())\n",
    "#cleaned_data = cleaned_data.fillna(\"\")  # Replace NaN with an appropriate value or an empty string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through all files in the clean output folder\n",
    "for file in os.listdir(clean_output_folder):\n",
    "    if file.endswith('.txt'):\n",
    "        # Read the cleaned data from the .txt file\n",
    "        cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
    "\n",
    "        # Check for null values in the DataFrame\n",
    "        null_values = cleaned_data.isnull().sum()\n",
    "\n",
    "        # Print information about null values\n",
    "        print(f\"Null values in {file}:\\n{null_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201001_201003\n"
     ]
    }
   ],
   "source": [
    "# Set up BigQuery client\n",
    "client = bigquery.Client(project=gbq_proj_id, credentials=credentials)\n",
    "\n",
    "# Loop through all files in the clean output folder\n",
    "#for file in ['transArchive_201605.txt', 'transArchive_201407_201409.txt', 'transArchive_201001_201003.txt']: #os.listdir(clean_output_folder):\n",
    "for file in os.listdir(clean_output_folder):\n",
    "    if file.endswith('.txt'):\n",
    "        # Read the cleaned data from the .txt file\n",
    "        cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t', low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "##### Adding this to force loacal to FLOAT\n",
    "\n",
    "\n",
    "\n",
    "        cleaned_data['local'] = cleaned_data['local'].replace('\\\\N', pd.NA)\n",
    "\n",
    "        #cleaned_data['local'] = cleaned_data['local'].astype(float)\n",
    "\n",
    "        cleaned_data['local'] = pd.to_numeric(cleaned_data['local'], errors='coerce').astype(float)\n",
    "\n",
    "##### Adding this to force loacal to FLOAT\n",
    "        #schema = [\n",
    "            #bigquery.SchemaField(\"local\", \"FLOAT\"),  # Change \"FLOAT\" to the desired datatype\n",
    "            # Add other schema fields as needed\n",
    "            # ...\n",
    "        #]\n",
    "##### Adding this to force loacal to FLOAT\n",
    "\n",
    "\n",
    "\n",
    "                # Create a BigQuery table name using the file name\n",
    "        table_name = file.replace('.txt', '')\n",
    "\n",
    "            # Create the BigQuery table\n",
    "        table_ref = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "        job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "        try:\n",
    "            # Upload data to BigQuery\n",
    "            cleaned_data.to_gbq(destination_table=table_ref, project_id=gbq_proj_id, if_exists=\"replace\")\n",
    "            print(f\"Data uploaded to BigQuery table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading data to BigQuery table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaned_data['charflag'] = cleaned_data['charflag'].astype(str)\n",
    "cleaned_data['memType'] = cleaned_data['memType'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "str(\"memType\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(cleaned_data[\"memType\"].dtype)\n",
    "print(cleaned_data[\"memType\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problematic_column_name = cleaned_data.columns[32]\n",
    "print(f\"Problematic column name: {problematic_column_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up BigQuery client\n",
    "client = bigquery.Client(project=gbq_proj_id, credentials=credentials)\n",
    "\n",
    "# Loop through all files in the clean output folder\n",
    "for file in ['transArchive_201204_201206.txt']: #os.listdir(clean_output_folder):\n",
    "    if file.endswith('.txt'):\n",
    "        # Read the cleaned data from the .txt file\n",
    "        cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
    "\n",
    "        #break\n",
    "        ### TRYING this to set data type in coluns, below\n",
    "\n",
    "\n",
    "                # Handle mixed data types in 'charflag' column\n",
    "        cleaned_data['charflag'] = cleaned_data['charflag'].astype(str)\n",
    "\n",
    "        # Check and handle data types as needed\n",
    "        cleaned_data = cleaned_data.astype({\n",
    "            \"datetime\": \"datetime64[ns]\",\n",
    "            \"register_no\": \"float64\",\n",
    "            # ... Specify data types for other columns as needed\n",
    "        })\n",
    "\n",
    "        ### TRYING this to set data type in coluns^\n",
    "\n",
    "\n",
    "        # Create a BigQuery table name using the file name\n",
    "        table_name = file.replace('.txt', '')\n",
    "\n",
    "        # Define the BigQuery schema\n",
    "        schema = [\n",
    "            {\"name\": col, \"type\": cleaned_data[col].dtype.name.lower()}\n",
    "            for col in cleaned_data.columns\n",
    "        ]\n",
    "\n",
    "### Or maybe something like this, but why not just define this in the above Schema? - Ask John\n",
    "                schema = [\n",
    "            {\"name\": col, \"type\": \"STRING\"} if col in datetime_columns else\n",
    "            {\"name\": col, \"type\": cleaned_data[col].dtype.name.lower()}\n",
    "            for col in cleaned_data.columns\n",
    "        ]\n",
    "# delete the above section if better to correct in the Schema...........................\n",
    "\n",
    "        # Create the BigQuery table\n",
    "        table_ref = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "        job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "        try:\n",
    "            # Upload data to BigQuery\n",
    "            cleaned_data.to_gbq(destination_table=table_ref, project_id=gbq_proj_id, if_exists=\"replace\")\n",
    "            print(f\"Data uploaded to BigQuery table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading data to BigQuery table {table_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
