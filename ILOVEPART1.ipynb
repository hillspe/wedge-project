{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from zipfile import ZipFile\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path where your ZIP files are located locally\n",
    "# Use a raw string for the path\n",
    "#directory_path = \"/Users/biancabostrom/Documents/ADA/Wedge Project/WedgeZipOfZips_Big\"\n",
    "directory_path = r'C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big'\n",
    "output_folder = 'extracted_zips_big'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over all the files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Create a folder for each ZIP file\n",
    "        folder_name = os.path.splitext(filename)[0]\n",
    "        extract_path = os.path.join(output_folder)\n",
    "\n",
    "        # Print the file path for debugging\n",
    "        print(f\"Attempting to extract: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all the contents into the directory\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "print(\"All files extracted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning files: headers, delimeters, nulls and quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "# loop though all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        if file.endswith('.csv'):\n",
    "            with open(full_path,'r') as f:\n",
    "                first_line = f.readline().strip()\n",
    "\n",
    "            with open(full_path,'r') as f:\n",
    "                content = f.read()\n",
    "            # check if the file likly has headers based on the first line\n",
    "            if not first_line.startswith('\"datetime\"') and not first_line.startswith('datetime'):\n",
    "                content = ','.join(correct_headers) + '\\n' + content\n",
    "\n",
    "            content = content.replace('\\\"','inch')\n",
    "\n",
    "            with open(full_path,'w') as f:\n",
    "                f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bianca's info\n",
    "#service_path = \"/Users/biancabostrom/Documents/ADA/Wedge\\ Project/wedge-404400-cb3a632effa5.json\"\n",
    "#service_file = 'wedge-404400-cb3a632effa5.json' \n",
    "#gbq_proj_id = \"wedge-404400\" \n",
    "#gbq_dataset_id = \"wedge_data\"\n",
    "#credentials = service_account.Credentials.from_service_account_file(\"/Users/biancabostrom/Documents/ADA/Wedge Project/wedge-404400-cb3a632effa5.json\")\n",
    "\n",
    "# Spencer's info\n",
    "service_path = r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\"\n",
    "service_file = 'leafy-sunrise-403222-f51fcd80b921.json' # change this to your authentication information  \n",
    "gbq_proj_id = \"leafy-sunrise-403222\" # change this to your project. \n",
    "gbq_dataset_id = \"wedge_data\"\n",
    "credentials = service_account.Credentials.from_service_account_file(r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\")\n",
    "\n",
    "private_key =service_path + service_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50000\n",
    "\n",
    "def drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "    \n",
    "def clean_dataframe(df):\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice'\n",
    "        , 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype'\n",
    "        , 'voided', 'percentDiscount', 'itemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag'\n",
    "        , 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'reciept', 'card_no', 'store', 'branch', 'match_id'\n",
    "        ,'trans_id'\n",
    "    ]\n",
    "\n",
    "    boolean_columns = [ 'memType', 'staff', 'batchHeaderID', 'display']\n",
    "\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col]  = df[col].astype(str)\n",
    "            df [col] = df [col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df[float_columns] = df[float_columns].fillna(0)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    \n",
    "    replace_strings = [\"\\\\n\", \"\\\\\\\\\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "            #df[col] = df[col].str.replace('\\\\\\\"', '', regex=False)\n",
    "\n",
    "    for col in df.columns: # added these three lines trying to problem solve next chuck for gbq file path\n",
    "        if col == 'charflag' and col in df.columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    df = df.applymap(lambda x: None if x == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: transArchive_201001_201003.csv\n",
      "detected delimiter: ,\n",
      "table 'leafy-sunrise-403222.wedge_data.transArchive_201001_201003' not found, skipping deletion.\n",
      "table 'leafy-sunrise-403222.wedge_data.transArchive_201001_201003' not found, skipping deletion.\n",
      "reading csv file in chunks: transArchive_201001_201003.csv...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale',\\n       'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax',\\n       'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount',\\n       'discountable', 'discounttype', 'voided', 'percentDiscount', 'itemQtty',\\n       'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag',\\n       'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'reciept',\\n       'card_no', 'store', 'branch', 'match_id', 'trans_id'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreading csv file in chunks: \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, chunk_df \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunk_iter):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# clean the DF\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     chunk_df \u001b[39m=\u001b[39m clean_dataframe(chunk_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# modify the field names to comply with the gbq rules\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     chunk_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [col\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m chunk_df\u001b[39m.\u001b[39mcolumns]\n",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         df[col] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_numeric(df[col], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m df[float_columns] \u001b[39m=\u001b[39m df[float_columns]\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m boolean_columns:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X26sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3461\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3459\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3460\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3461\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3463\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1314\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 1314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_read_indexer(keyarr, indexer, axis)\n\u001b[0;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m needs_i8_conversion(ax\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1317\u001b[0m     ax, (IntervalIndex, CategoricalIndex)\n\u001b[0;32m   1318\u001b[0m ):\n\u001b[0;32m   1319\u001b[0m     \u001b[39m# For CategoricalIndex take instead of reindex to preserve dtype.\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m     \u001b[39m#  For IntervalIndex this is to map integers to the Intervals they match to.\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     keyarr \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1374\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1372\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   1373\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 1374\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1376\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m   1377\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale',\\n       'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax',\\n       'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount',\\n       'discountable', 'discounttype', 'voided', 'percentDiscount', 'itemQtty',\\n       'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag',\\n       'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'reciept',\\n       'card_no', 'store', 'branch', 'match_id', 'trans_id'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#duplicate code to play with\n",
    "\n",
    "# move through all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"detected delimiter: {delimiter}\")\n",
    "\n",
    "            # reading csv with correct handling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('.csv', '')\n",
    "\n",
    "            # drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    \n",
    "            # Construct the fully-qualified table_id without \".csv\" extension\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            print(f\"reading csv file in chunks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # clean the DF\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                # modify the field names to comply with the gbq rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # for the first chunk create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # for subsequent chunks, append to the table \n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # clean the chunk from memory\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: transArchive_201001_201003.csv\n",
      "detected delimiter: ,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "table_id must be a fully-qualified ID in standard SQL format, e.g., \"project.dataset.table_id\", got leafy-sunrise-403222.wedge_data.transArchive_201001_201003.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m table_name \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#drop the table if it exists\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m client \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39mClient(credentials\u001b[39m=\u001b[39mcredentials, project\u001b[39m=\u001b[39mgbq_proj_id)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Construct the fully-qualified table_id\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m table_id \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mgbq_proj_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mgbq_dataset_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mtable_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     client\u001b[39m.\u001b[39;49mdelete_table(table_id)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeleted table \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtable_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mexcept\u001b[39;00m NotFound:\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:1836\u001b[0m, in \u001b[0;36mClient.delete_table\u001b[1;34m(self, table, retry, timeout, not_found_ok)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdelete_table\u001b[39m(\n\u001b[0;32m   1805\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1806\u001b[0m     table: Union[Table, TableReference, TableListItem, \u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1809\u001b[0m     not_found_ok: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1810\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Delete a table\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m \n\u001b[0;32m   1813\u001b[0m \u001b[39m    See\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1834\u001b[0m \u001b[39m            when deleting the table.\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1836\u001b[0m     table \u001b[39m=\u001b[39m _table_arg_to_table_ref(table, default_project\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproject)\n\u001b[0;32m   1837\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(table, TableReference):\n\u001b[0;32m   1838\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to get TableReference for table \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(table))\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\table.py:2937\u001b[0m, in \u001b[0;36m_table_arg_to_table_ref\u001b[1;34m(value, default_project)\u001b[0m\n\u001b[0;32m   2932\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Helper to convert a string or Table to TableReference.\u001b[39;00m\n\u001b[0;32m   2933\u001b[0m \n\u001b[0;32m   2934\u001b[0m \u001b[39mThis function keeps TableReference and other kinds of objects unchanged.\u001b[39;00m\n\u001b[0;32m   2935\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2936\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 2937\u001b[0m     value \u001b[39m=\u001b[39m TableReference\u001b[39m.\u001b[39;49mfrom_string(value, default_project\u001b[39m=\u001b[39;49mdefault_project)\n\u001b[0;32m   2938\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, (Table, TableListItem)):\n\u001b[0;32m   2939\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mreference\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\table.py:263\u001b[0m, in \u001b[0;36mTableReference.from_string\u001b[1;34m(cls, table_id, default_project)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Construct a table reference from table ID string.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39m        standard SQL format.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetReference\n\u001b[0;32m    259\u001b[0m (\n\u001b[0;32m    260\u001b[0m     output_project_id,\n\u001b[0;32m    261\u001b[0m     output_dataset_id,\n\u001b[0;32m    262\u001b[0m     output_table_id,\n\u001b[1;32m--> 263\u001b[0m ) \u001b[39m=\u001b[39m _helpers\u001b[39m.\u001b[39;49m_parse_3_part_id(\n\u001b[0;32m    264\u001b[0m     table_id, default_project\u001b[39m=\u001b[39;49mdefault_project, property_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtable_id\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    265\u001b[0m )\n\u001b[0;32m    267\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    268\u001b[0m     DatasetReference(output_project_id, output_dataset_id), output_table_id\n\u001b[0;32m    269\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\_helpers.py:763\u001b[0m, in \u001b[0;36m_parse_3_part_id\u001b[1;34m(full_id, default_project, property_name)\u001b[0m\n\u001b[0;32m    760\u001b[0m parts \u001b[39m=\u001b[39m _split_id(full_id)\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parts) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parts) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    764\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{property_name}\u001b[39;00m\u001b[39m must be a fully-qualified ID in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstandard SQL format, e.g., \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mproject.dataset.\u001b[39m\u001b[39m{property_name}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    766\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(full_id, property_name\u001b[39m=\u001b[39mproperty_name)\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parts) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m default_project:\n\u001b[0;32m    770\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    771\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen default_project is not set, \u001b[39m\u001b[39m{property_name}\u001b[39;00m\u001b[39m must be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfully-qualified ID in standard SQL format, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m         )\n\u001b[0;32m    776\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: table_id must be a fully-qualified ID in standard SQL format, e.g., \"project.dataset.table_id\", got leafy-sunrise-403222.wedge_data.transArchive_201001_201003.csv"
     ]
    }
   ],
   "source": [
    "# move through all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"detected delimiter: {delimiter}\")\n",
    "\n",
    "            #reading csv with correct handeling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter,quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('data.csv', '')\n",
    "\n",
    "            #drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    \n",
    "                # Construct the fully-qualified table_id\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            print(f\"reading csv file in chucks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                #clean the DF\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                #modify the field names to comply with the gbq rules\n",
    "                chunk_df.columns = [col.lower().replace(';','') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # for the first chuck create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # for subsuquent chunks, append to the table \n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df # clean the chunk from memory\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for delimiters first\n",
    "delimiters = dict()\n",
    "\n",
    "# Start by reading in all the files again.\n",
    "for this_folder in os.listdir(output_folder):\n",
    "    folder_path = os.path.join(output_folder, this_folder)\n",
    "\n",
    "    # Check if the entry is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.csv'):  # Check if the file is a CSV file\n",
    "                input_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                # Read the content of the file\n",
    "                with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n",
    "                    # Read only the first line to determine the delimiter\n",
    "                    first_line = input_file.readline()\n",
    "\n",
    "                    dialect = csv.Sniffer().sniff(sample=first_line, delimiters=[\",\", \";\", \"\\t\"])\n",
    "                    delimiters[file_name] = dialect.delimiter\n",
    "\n",
    "                    print(\" \".join([\"It looks like\",\n",
    "                                    file_name,\n",
    "                                    \"in folder\",\n",
    "                                    this_folder,\n",
    "                                    \"has delimiter\",\n",
    "                                    dialect.delimiter,\n",
    "                                    \".\"]))\n",
    "\n",
    "                # You can add the rest of your processing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each file and replace the delimters to \",\" in the files that don't\n",
    "\n",
    "# Clean the files\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'a') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file_path = os.path.join(zip_folder, this_zf, file_name)\n",
    "            \n",
    "            # Read the content of the file\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                content = input_file.read().decode(\"utf-8\")\n",
    "            \n",
    "            # Check if the delimiter is not a comma\n",
    "            if delimiters[file_name] != \",\": # might be useful\n",
    "                # Replace the delimiter with a comma # might be useful\n",
    "                content = content.replace(delimiters[file_name], \",\")\n",
    "\n",
    "                # Write the modified content back to the file\n",
    "                zf.writestr(file_name, content.encode(\"utf-8\"))\n",
    "\n",
    "                print(f\"File {file_name} has been cleaned.\")\n",
    "\n",
    "print(\"Done cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace empty values with \"null\" #\n",
    "\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'a') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file_path = os.path.join(zip_folder, this_zf, file_name)\n",
    "\n",
    "            # Read the content of the file\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                content = input_file.read().decode(\"utf-8\")\n",
    "\n",
    "            # Identify and replace null values (assuming nulls are represented as an empty string \"\")\n",
    "            content = content.replace('\"\"', 'null')\n",
    "\n",
    "            # Write the modified content back to the file\n",
    "            zf.writestr(file_name, content.encode(\"utf-8\"))\n",
    "\n",
    "            print(f\"Null values handled in file {file_name}.\")\n",
    "\n",
    "print(\"Done checking and handling null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to GBQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
