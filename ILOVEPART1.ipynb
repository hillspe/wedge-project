{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import glob\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace 'your_zip_file.zip' with the actual name of your zip file\n",
    "#zip_file_path = 'your_zip_file.zip'\n",
    "#extract_folder = 'your_extraction_folder'  # Replace with the desired extraction folder\n",
    "\n",
    "#with ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    #zip_ref.extractall(extract_folder)\n",
    "\n",
    "#print(f\"Contents extracted to {extract_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_folder = \"/Users/biancabostrom/Documents/ADA/Wedge Project/WedgeZipOfZips\"\n",
    "extract_destination = \"/Users/biancabostrom/Documents/ADA/Wedge Project/ExtractedFiles\"\n",
    "\n",
    "zip_files = [f for f in os.listdir(zip_folder) if f.endswith('.zip')]\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(zip_folder, zip_file)\n",
    "\n",
    "    with ZipFile(zip_file_path, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file = zf.open(file_name, 'r')\n",
    "            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "            for idx, line in enumerate(input_file):\n",
    "                print(line)\n",
    "                if idx > 3:\n",
    "                    break\n",
    "\n",
    "            input_file.close()  # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path where your ZIP files are located\n",
    "directory_path = r'C:\\path\\to\\your\\folder'  # Use a raw string for the path\n",
    " \n",
    "# Iterate over all the files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    " \n",
    "        # Print the file path for debugging\n",
    "        print(f\"Attempting to extract: {file_path}\")\n",
    " \n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all the contents into the directory\n",
    "                zip_ref.extractall(directory_path)\n",
    "                print(f\"Extracted {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    " \n",
    "print(\"All files extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning files: headers, delimeters, nulls and quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each file to check and add headers if different \n",
    "\n",
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'a') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file_path = os.path.join(zip_folder, this_zf, file_name)\n",
    "\n",
    "            # Read the content of the file\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                content = input_file.read().decode(\"utf-8\")\n",
    "\n",
    "            # Check if headers are present\n",
    "            has_headers = all(header in content for header in correct_headers)\n",
    "\n",
    "            if not has_headers:\n",
    "                # Headers are missing, add them\n",
    "                content = \",\".join(correct_headers) + \"\\n\" + content\n",
    "\n",
    "                # Write the modified content back to the file\n",
    "                zf.writestr(file_name, content.encode(\"utf-8\"))\n",
    "\n",
    "                print(f\"Headers added to file {file_name}.\")\n",
    "\n",
    "            print(f\"File {file_name} is good.\")\n",
    "\n",
    "print(\"Done checking and adding headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for delimters\n",
    "\n",
    "delimiters = dict() \n",
    "\n",
    "# Start by reading in all the files again.\n",
    "\n",
    "for this_zf in zip_files :\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files :\n",
    "            input_file = zf.open(file_name,'r')\n",
    "            input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "            \n",
    "            dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                      delimiters=[\",\",\";\",\"\\t\"])\n",
    "            \n",
    "            delimiters[file_name] = dialect.delimiter\n",
    "            \n",
    "            print(\" \".join([\"It looks like\",\n",
    "                           file_name,\n",
    "                           \"has delimiter\",\n",
    "                           dialect.delimiter,\n",
    "                           \".\"]))\n",
    "\n",
    "            input_file.close() # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each file and replace the delimters to \",\" in the files that don't\n",
    "\n",
    "# Clean the files\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'a') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file_path = os.path.join(zip_folder, this_zf, file_name)\n",
    "            \n",
    "            # Read the content of the file\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                content = input_file.read().decode(\"utf-8\")\n",
    "            \n",
    "            # Check if the delimiter is not a comma\n",
    "            if delimiters[file_name] != \",\":\n",
    "                # Replace the delimiter with a comma\n",
    "                content = content.replace(delimiters[file_name], \",\")\n",
    "\n",
    "                # Write the modified content back to the file\n",
    "                zf.writestr(file_name, content.encode(\"utf-8\"))\n",
    "\n",
    "                print(f\"File {file_name} has been cleaned.\")\n",
    "\n",
    "print(\"Done cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace empty values with \"null\"\n",
    "\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(zip_folder, this_zf), 'a') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file_path = os.path.join(zip_folder, this_zf, file_name)\n",
    "\n",
    "            # Read the content of the file\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                content = input_file.read().decode(\"utf-8\")\n",
    "\n",
    "            # Identify and replace null values (assuming nulls are represented as an empty string \"\")\n",
    "            content = content.replace('\"\"', 'null')\n",
    "\n",
    "            # Write the modified content back to the file\n",
    "            zf.writestr(file_name, content.encode(\"utf-8\"))\n",
    "\n",
    "            print(f\"Null values handled in file {file_name}.\")\n",
    "\n",
    "print(\"Done checking and handling null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_path = \"/Users/biancabostrom/Documents/ADA/Wedge\\ Project/wedge-404400-cb3a632effa5.json\"\n",
    "service_file = 'wedge-404400-cb3a632effa5.json' \n",
    "gbq_proj_id = \"wedge-404400\" \n",
    "gbq_dataset_id = \"wedge_data\"\n",
    "\n",
    "\n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\"/Users/biancabostrom/Documents/ADA/Wedge Project/wedge-404400-cb3a632effa5.json\")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the Pandas function to_gbq to upload your data to GBQ.\n",
    "\n",
    "#Iterate through each CSV file in the folder\n",
    "\n",
    "for file_name in os.listdir(zip_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(zip_folder, file_name)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Define the BigQuery table ID in the format 'project.dataset.table'\n",
    "        table_id = f'your_project.your_dataset.{file_name.replace(\".csv\", \"\")}'\n",
    "\n",
    "        # Upload the DataFrame to BigQuery using to_gbq\n",
    "        df.to_gbq(destination_table=table_id, project_id='your_project', if_exists='replace')\n",
    "\n",
    "print(\"Upload complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
