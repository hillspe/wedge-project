{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from zipfile import ZipFile\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path where your ZIP files are located locally\n",
    "# Use a raw string for the path\n",
    "#directory_path = \"/Users/biancabostrom/Documents/ADA/Wedge Project/WedgeZipOfZips_Big\"\n",
    "directory_path = r'C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big'\n",
    "output_folder = 'extracted_zips_big'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201001_201003.zip\n",
      "Extracted transArchive_201001_201003.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201004_201006.zip\n",
      "Extracted transArchive_201004_201006.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201007_201009.zip\n",
      "Extracted transArchive_201007_201009.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201010_201012.zip\n",
      "Extracted transArchive_201010_201012.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201101_201103.zip\n",
      "Extracted transArchive_201101_201103.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201104.zip\n",
      "Extracted transArchive_201104.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201105.zip\n",
      "Extracted transArchive_201105.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201106.zip\n",
      "Extracted transArchive_201106.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201107_201109.zip\n",
      "Extracted transArchive_201107_201109.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201110_201112.zip\n",
      "Extracted transArchive_201110_201112.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201201_201203.zip\n",
      "Extracted transArchive_201201_201203.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201201_201203_inactive.zip\n",
      "Extracted transArchive_201201_201203_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201204_201206.zip\n",
      "Extracted transArchive_201204_201206.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201204_201206_inactive.zip\n",
      "Extracted transArchive_201204_201206_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201207_201209.zip\n",
      "Extracted transArchive_201207_201209.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201207_201209_inactive.zip\n",
      "Extracted transArchive_201207_201209_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201210_201212.zip\n",
      "Extracted transArchive_201210_201212.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201210_201212_inactive.zip\n",
      "Extracted transArchive_201210_201212_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201301_201303.zip\n",
      "Extracted transArchive_201301_201303.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201301_201303_inactive.zip\n",
      "Extracted transArchive_201301_201303_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201304_201306.zip\n",
      "Extracted transArchive_201304_201306.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201304_201306_inactive.zip\n",
      "Extracted transArchive_201304_201306_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201307_201309.zip\n",
      "Extracted transArchive_201307_201309.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201307_201309_inactive.zip\n",
      "Extracted transArchive_201307_201309_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201310_201312.zip\n",
      "Extracted transArchive_201310_201312.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201310_201312_inactive.zip\n",
      "Extracted transArchive_201310_201312_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201401_201403.zip\n",
      "Extracted transArchive_201401_201403.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201401_201403_inactive.zip\n",
      "Extracted transArchive_201401_201403_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201404_201406.zip\n",
      "Extracted transArchive_201404_201406.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201404_201406_inactive.zip\n",
      "Extracted transArchive_201404_201406_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201407_201409.zip\n",
      "Extracted transArchive_201407_201409.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201407_201409_inactive.zip\n",
      "Extracted transArchive_201407_201409_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201410_201412.zip\n",
      "Extracted transArchive_201410_201412.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201410_201412_inactive.zip\n",
      "Extracted transArchive_201410_201412_inactive.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201501_201503.zip\n",
      "Extracted transArchive_201501_201503.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201504_201506.zip\n",
      "Extracted transArchive_201504_201506.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201507_201509.zip\n",
      "Extracted transArchive_201507_201509.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201510.zip\n",
      "Extracted transArchive_201510.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201511.zip\n",
      "Extracted transArchive_201511.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201512.zip\n",
      "Extracted transArchive_201512.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201601.zip\n",
      "Extracted transArchive_201601.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201602.zip\n",
      "Extracted transArchive_201602.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201603.zip\n",
      "Extracted transArchive_201603.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201604.zip\n",
      "Extracted transArchive_201604.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201605.zip\n",
      "Extracted transArchive_201605.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201606.zip\n",
      "Extracted transArchive_201606.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201607.zip\n",
      "Extracted transArchive_201607.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201608.zip\n",
      "Extracted transArchive_201608.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201609.zip\n",
      "Extracted transArchive_201609.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201610.zip\n",
      "Extracted transArchive_201610.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201611.zip\n",
      "Extracted transArchive_201611.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201612.zip\n",
      "Extracted transArchive_201612.zip to extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201701.zip\n",
      "Extracted transArchive_201701.zip to extracted_zips_big\n",
      "All files extracted.\n"
     ]
    }
   ],
   "source": [
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over all the files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Create a folder for each ZIP file\n",
    "        folder_name = os.path.splitext(filename)[0]\n",
    "        extract_path = os.path.join(output_folder)\n",
    "\n",
    "        # Print the file path for debugging\n",
    "        print(f\"Attempting to extract: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all the contents into the directory\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "print(\"All files extracted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning files: headers, delimeters, nulls and quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "# loop though all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        if file.endswith('.csv'):\n",
    "            with open(full_path,'r') as f:\n",
    "                first_line = f.readline().strip()\n",
    "\n",
    "            with open(full_path,'r') as f:\n",
    "                content = f.read()\n",
    "            # check if the file likely has headers based on the first line\n",
    "            if not first_line.startswith('\"datetime\"') and not first_line.startswith('datetime'):\n",
    "                content = ','.join(correct_headers) + '\\n' + content\n",
    "\n",
    "            #content = content.replace('\\\"','inch')\n",
    "\n",
    "            #with open(full_path,'w') as f:``\n",
    "                #f.write(content)\n",
    "\n",
    "            content = '\\n'.join(['inch' + line.strip('\\\"') + 'inch' for line in content.split('\\n')])\n",
    "\n",
    "            with open(full_path, 'w') as f:\n",
    "                f.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bianca's info\n",
    "#service_path = \"/Users/biancabostrom/Documents/ADA/Wedge\\ Project/wedge-404400-cb3a632effa5.json\"\n",
    "#service_file = 'wedge-404400-cb3a632effa5.json' \n",
    "#gbq_proj_id = \"wedge-404400\" \n",
    "#gbq_dataset_id = \"wedge_data\"\n",
    "#credentials = service_account.Credentials.from_service_account_file(\"/Users/biancabostrom/Documents/ADA/Wedge Project/wedge-404400-cb3a632effa5.json\")\n",
    "\n",
    "# Spencer's info\n",
    "service_path = r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\"\n",
    "service_file = 'leafy-sunrise-403222-f51fcd80b921.json' # change this to your authentication information  \n",
    "gbq_proj_id = \"leafy-sunrise-403222\" # change this to your project. \n",
    "gbq_dataset_id = \"wedge_data\"\n",
    "credentials = service_account.Credentials.from_service_account_file(r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\")\n",
    "\n",
    "private_key = service_path + service_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    {\"name\": \"datetime\", \"type\": \"TIMESTAMP\"},     # 1\n",
    "    {\"name\": \"register_no\", \"type\": \"FLOAT\"},      # 2\n",
    "    {\"name\": \"emp_no\", \"type\": \"FLOAT\"},           # 3\n",
    "    {\"name\": \"trans_no\", \"type\": \"FLOAT\"},         # 4\n",
    "    {\"name\": \"upc\", \"type\": \"STRING\"},             # 5\n",
    "    {\"name\": \"description\", \"type\": \"STRING\"},     # 6\n",
    "    {\"name\": \"trans_type\", \"type\": \"STRING\"},      # 7\n",
    "    {\"name\": \"trans_subtype\", \"type\": \"STRING\"},   # 8\n",
    "    {\"name\": \"trans_status\", \"type\": \"STRING\"},    # 9\n",
    "    {\"name\": \"department\", \"type\": \"FLOAT\"},       # 10\n",
    "    {\"name\": \"quantity\", \"type\": \"FLOAT\"},         # 11\n",
    "    {\"name\": \"Scale\", \"type\": \"FLOAT\"},            # 12\n",
    "    {\"name\": \"cost\", \"type\": \"FLOAT\"},             # 13\n",
    "    {\"name\": \"unitPrice\", \"type\": \"FLOAT\"},        # 14\n",
    "    {\"name\": \"total\", \"type\": \"FLOAT\"},            # 15\n",
    "    {\"name\": \"regPrice\", \"type\": \"FLOAT\"},         # 16\n",
    "    {\"name\": \"altPrice\", \"type\": \"FLOAT\"},         # 17\n",
    "    {\"name\": \"tax\", \"type\": \"FLOAT\"},              # 18\n",
    "    {\"name\": \"taxexempt\", \"type\": \"FLOAT\"},        # 19\n",
    "    {\"name\": \"foodstamp\", \"type\": \"FLOAT\"},        # 20\n",
    "    {\"name\": \"wicable\", \"type\": \"FLOAT\"},          # 21\n",
    "    {\"name\": \"discount\", \"type\": \"FLOAT\"},         # 22\n",
    "    {\"name\": \"memDiscount\", \"type\": \"FLOAT\"},      # 23\n",
    "    {\"name\": \"discountable\", \"type\": \"FLOAT\"},     # 24\n",
    "    {\"name\": \"discounttype\", \"type\": \"FLOAT\"},     # 25\n",
    "    {\"name\": \"voided\", \"type\": \"FLOAT\"},           # 26\n",
    "    {\"name\": \"percentDiscount\", \"type\": \"FLOAT\"},  # 27\n",
    "    {\"name\": \"ItemQtty\", \"type\": \"FLOAT\"},         # 28\n",
    "    {\"name\": \"volDiscType\", \"type\": \"FLOAT\"},      # 29\n",
    "    {\"name\": \"volume\", \"type\": \"FLOAT\"},           # 30\n",
    "    {\"name\": \"VolSpecial\", \"type\": \"FLOAT\"},       # 31\n",
    "    {\"name\": \"mixMatch\", \"type\": \"FLOAT\"},         # 32\n",
    "    {\"name\": \"matched\", \"type\": \"FLOAT\"},          # 33\n",
    "    {\"name\": \"memType\", \"type\": \"BOOLEAN\"},        # 34\n",
    "    {\"name\": \"staff\", \"type\": \"BOOLEAN\"},          # 35\n",
    "    {\"name\": \"numflag\", \"type\": \"FLOAT\"},          # 36\n",
    "    {\"name\": \"itemstatus\", \"type\": \"FLOAT\"},       # 37\n",
    "    {\"name\": \"tenderstatus\", \"type\": \"FLOAT\"},     # 38\n",
    "    {\"name\": \"charflag\", \"type\": \"STRING\"},        # 39\n",
    "    {\"name\": \"varflag\", \"type\": \"FLOAT\"},          # 40\n",
    "    {\"name\": \"batchHeaderID\", \"type\": \"BOOLEAN\"},  # 41\n",
    "    {\"name\": \"local\", \"type\": \"FLOAT\"},            # 42\n",
    "    {\"name\": \"organic\", \"type\": \"FLOAT\"},          # 43\n",
    "    {\"name\": \"display\", \"type\": \"BOOLEAN\"},        # 44\n",
    "    {\"name\": \"receipt\", \"type\": \"FLOAT\"},          # 45\n",
    "    {\"name\": \"card_no\", \"type\": \"FLOAT\"},          # 46\n",
    "    {\"name\": \"store\", \"type\": \"FLOAT\"},            # 47\n",
    "    {\"name\": \"branch\", \"type\": \"FLOAT\"},           # 48\n",
    "    {\"name\": \"match_id\", \"type\": \"FLOAT\"},         # 49\n",
    "    {\"name\": \"trans_id\", \"type\": \"FLOAT\"}          # 50\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "    \n",
    "def clean_dataframe(df):\n",
    "    # Original column names\n",
    "    original_columns = df.columns.tolist()\n",
    "\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice'\n",
    "        , 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype'\n",
    "        , 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag'\n",
    "        , 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'receipt', 'card_no', 'store', 'branch', 'match_id'\n",
    "        ,'trans_id'\n",
    "    ]\n",
    "\n",
    "    boolean_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    # Replace the modified column names with the original ones\n",
    "    df.columns = original_columns\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col] = df[col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Check if the column exists before filling NaN values\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    replace_strings = [\"\\\\n\", \"\\\\\\\\\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    for col in ['ItemQtty', 'reciept']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace('\"', '', regex=False)\n",
    "\n",
    "    # Print the first 20 rows for inspection\n",
    "    print(\"First 20 rows after cleaning:\")\n",
    "    print(df.head(20))\n",
    "\n",
    "    df = df.applymap(lambda x: None if x == '' else x)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['inchdatetimeinch', 'inchregister_noinch', 'inchemp_noinch',\n",
      "       'inchtrans_noinch', 'inchupcinch', 'inchdescriptioninch',\n",
      "       'inchtrans_typeinch', 'inchtrans_subtypeinch', 'inchtrans_statusinch',\n",
      "       'inchdepartmentinch', 'inchquantityinch', 'inchScaleinch',\n",
      "       'inchcostinch', 'inchunitPriceinch', 'inchtotalinch',\n",
      "       'inchregPriceinch', 'inchaltPriceinch', 'inchtaxinch',\n",
      "       'inchtaxexemptinch', 'inchfoodstampinch', 'inchwicableinch',\n",
      "       'inchdiscountinch', 'inchmemDiscountinch', 'inchdiscountableinch',\n",
      "       'inchdiscounttypeinch', 'inchvoidedinch', 'inchpercentDiscountinch',\n",
      "       'inchItemQttyinch', 'inchvolDiscTypeinch', 'inchvolumeinch',\n",
      "       'inchVolSpecialinch', 'inchmixMatchinch', 'inchmatchedinch',\n",
      "       'inchmemTypeinch', 'inchstaffinch', 'inchnumflaginch',\n",
      "       'inchitemstatusinch', 'inchtenderstatusinch', 'inchcharflaginch',\n",
      "       'inchvarflaginch', 'inchbatchHeaderIDinch', 'inchlocalinch',\n",
      "       'inchorganicinch', 'inchdisplayinch', 'inchreceiptinch',\n",
      "       'inchcard_noinch', 'inchstoreinch', 'inchbranchinch',\n",
      "       'inchmatch_idinch', 'inchtrans_idinch'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datetime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mColumns in the DataFrame:\u001b[39m\u001b[39m\"\u001b[39m, df\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39;49m\u001b[39mdatetime\u001b[39;49m\u001b[39m'\u001b[39;49m], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datetime'"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: transArchive_201001_201003.csv\n",
      "Detected delimiter: ,\n",
      "table 'leafy-sunrise-403222.wedge_data.transArchive_201001_201003' not found, skipping deletion.\n",
      "Table 'leafy-sunrise-403222.wedge_data.transArchive_201001_201003' not found, skipping deletion.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datetime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTable \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtable_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not found, skipping deletion.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Clean the DataFrame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m df \u001b[39m=\u001b[39m clean_dataframe(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Modify the field names to comply with the gbq rules\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [col\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns]\n",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39;49m\u001b[39mdatetime\u001b[39;49m\u001b[39m'\u001b[39;49m], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m replace_strings \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNULL\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#Y103sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m df\u001b[39m.\u001b[39mreplace(replace_strings, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datetime'"
     ]
    }
   ],
   "source": [
    "# move through all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "\n",
    "            # Reading CSV with correct handling of quoted fields\n",
    "            df = pd.read_csv(full_path, delimiter=delimiter, quotechar='\"', dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('.csv', '')\n",
    "\n",
    "            # Drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "\n",
    "            # Construct the fully-qualified table_id without \".csv\" extension\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"Deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"Table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            # Clean the DataFrame\n",
    "            df = clean_dataframe(df)\n",
    "\n",
    "            # Modify the field names to comply with the gbq rules\n",
    "            df.columns = [col.lower().replace(';', '') for col in df.columns]\n",
    "\n",
    "            print(f\"Uploading {table_name} to BigQuery...\")\n",
    "            pandas_gbq.to_gbq(df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "            del df  # Clean the DataFrame from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi John - cells below are for reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code chunk\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "def drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "    \n",
    "def clean_dataframe(df):\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice'\n",
    "        , 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype'\n",
    "        , 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag'\n",
    "        , 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'receipt', 'card_no', 'store', 'branch', 'match_id'\n",
    "        ,'trans_id'\n",
    "    ]\n",
    "\n",
    "    boolean_columns = [ 'memType', 'staff', 'batchHeaderID', 'display']\n",
    "\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col]  = df[col].astype(str)\n",
    "            df[col] = df[col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df[float_columns] = df[float_columns].fillna(0)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    replace_strings = [\"\\\\n\", \"\\\\\\\\\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    for col in ['itemQtty', 'reciept']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace('\"', '', regex=False)\n",
    "\n",
    "    # Print the first 20 rows for inspection\n",
    "    print(\"First 20 rows after cleaning:\")\n",
    "    print(df.head(20))\n",
    "\n",
    "    df = df.applymap(lambda x: None if x == '' else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hold on to this while I test another chunk - more than likley delete\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "def drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "    \n",
    "def clean_dataframe(df):\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice'\n",
    "        , 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype'\n",
    "        , 'voided', 'percentDiscount', 'itemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag'\n",
    "        , 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'reciept', 'card_no', 'store', 'branch', 'match_id'\n",
    "        ,'trans_id'\n",
    "    ]\n",
    "\n",
    "    boolean_columns = [ 'memType', 'staff', 'batchHeaderID', 'display']\n",
    "\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "        # Check if columns exist before applying operations\n",
    "\n",
    "    # List of columns to check and clean\n",
    "    #columns_to_clean = ['itemQtty', 'reciept']\n",
    "\n",
    "   # for col in columns_to_clean:\n",
    "    #    if col in df.columns:\n",
    "     #       if col == 'charflag':\n",
    "      #          df[col] = df[col].str.strip()\n",
    "       #     elif col in ['itemQtty', 'reciept']:\n",
    "        #        df[col] = df[col].astype(str).str.replace('\"', '', regex=False)\n",
    "\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col]  = df[col].astype(str)\n",
    "            df [col] = df [col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df[float_columns] = df[float_columns].fillna(0)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    \n",
    "    replace_strings = [\"\\\\n\", \"\\\\\\\\\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "            #df[col] = df[col].str.replace('\\\\\\\"', '', regex=False)\n",
    "\n",
    "    for col in df.columns: # added these three lines trying to problem solve next chuck for gbq file path\n",
    "        if col == 'charflag' and col in df.columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    df = df.applymap(lambda x: None if x == '' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#duplicate code to play with\n",
    "\n",
    "# move through all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"detected delimiter: {delimiter}\")\n",
    "\n",
    "            # reading csv with correct handling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('.csv', '')\n",
    "\n",
    "            # drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    \n",
    "            # Construct the fully-qualified table_id without \".csv\" extension\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            print(f\"reading csv file in chunks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # clean the DF\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                # modify the field names to comply with the gbq rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # for the first chunk create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # for subsequent chunks, append to the table \n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # clean the chunk from memory\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move through all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"detected delimiter: {delimiter}\")\n",
    "\n",
    "            #reading csv with correct handeling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter,quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('data.csv', '')\n",
    "\n",
    "            #drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    \n",
    "                # Construct the fully-qualified table_id\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            print(f\"reading csv file in chucks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                #clean the DF\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                #modify the field names to comply with the gbq rules\n",
    "                chunk_df.columns = [col.lower().replace(';','') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # for the first chuck create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # for subsuquent chunks, append to the table \n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df # clean the chunk from memory\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
