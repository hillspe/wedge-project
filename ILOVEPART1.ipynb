{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from zipfile import ZipFile\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path where your ZIP files are located locally\n",
    "# Use a raw string for the path\n",
    "#directory_path = \"/Users/biancabostrom/Documents/ADA/Wedge Project/WedgeZipOfZips_Big\"\n",
    "directory_path = r'C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big'\n",
    "output_folder = 'data\\\\extracted_zips_big'\n",
    "clean_output_folder = 'data\\\\clean_csvs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract CSV's from zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201001_201003.zip\n",
      "Extracted transArchive_201001_201003.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201004_201006.zip\n",
      "Extracted transArchive_201004_201006.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201007_201009.zip\n",
      "Extracted transArchive_201007_201009.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201010_201012.zip\n",
      "Extracted transArchive_201010_201012.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201101_201103.zip\n",
      "Extracted transArchive_201101_201103.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201104.zip\n",
      "Extracted transArchive_201104.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201105.zip\n",
      "Extracted transArchive_201105.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201106.zip\n",
      "Extracted transArchive_201106.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201107_201109.zip\n",
      "Extracted transArchive_201107_201109.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201110_201112.zip\n",
      "Extracted transArchive_201110_201112.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201201_201203.zip\n",
      "Extracted transArchive_201201_201203.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201201_201203_inactive.zip\n",
      "Extracted transArchive_201201_201203_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201204_201206.zip\n",
      "Extracted transArchive_201204_201206.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201204_201206_inactive.zip\n",
      "Extracted transArchive_201204_201206_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201207_201209.zip\n",
      "Extracted transArchive_201207_201209.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201207_201209_inactive.zip\n",
      "Extracted transArchive_201207_201209_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201210_201212.zip\n",
      "Extracted transArchive_201210_201212.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201210_201212_inactive.zip\n",
      "Extracted transArchive_201210_201212_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201301_201303.zip\n",
      "Extracted transArchive_201301_201303.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201301_201303_inactive.zip\n",
      "Extracted transArchive_201301_201303_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201304_201306.zip\n",
      "Extracted transArchive_201304_201306.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201304_201306_inactive.zip\n",
      "Extracted transArchive_201304_201306_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201307_201309.zip\n",
      "Extracted transArchive_201307_201309.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201307_201309_inactive.zip\n",
      "Extracted transArchive_201307_201309_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201310_201312.zip\n",
      "Extracted transArchive_201310_201312.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201310_201312_inactive.zip\n",
      "Extracted transArchive_201310_201312_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201401_201403.zip\n",
      "Extracted transArchive_201401_201403.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201401_201403_inactive.zip\n",
      "Extracted transArchive_201401_201403_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201404_201406.zip\n",
      "Extracted transArchive_201404_201406.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201404_201406_inactive.zip\n",
      "Extracted transArchive_201404_201406_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201407_201409.zip\n",
      "Extracted transArchive_201407_201409.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201407_201409_inactive.zip\n",
      "Extracted transArchive_201407_201409_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201410_201412.zip\n",
      "Extracted transArchive_201410_201412.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201410_201412_inactive.zip\n",
      "Extracted transArchive_201410_201412_inactive.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201501_201503.zip\n",
      "Extracted transArchive_201501_201503.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201504_201506.zip\n",
      "Extracted transArchive_201504_201506.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201507_201509.zip\n",
      "Extracted transArchive_201507_201509.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201510.zip\n",
      "Extracted transArchive_201510.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201511.zip\n",
      "Extracted transArchive_201511.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201512.zip\n",
      "Extracted transArchive_201512.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201601.zip\n",
      "Extracted transArchive_201601.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201602.zip\n",
      "Extracted transArchive_201602.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201603.zip\n",
      "Extracted transArchive_201603.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201604.zip\n",
      "Extracted transArchive_201604.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201605.zip\n",
      "Extracted transArchive_201605.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201606.zip\n",
      "Extracted transArchive_201606.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201607.zip\n",
      "Extracted transArchive_201607.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201608.zip\n",
      "Extracted transArchive_201608.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201609.zip\n",
      "Extracted transArchive_201609.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201610.zip\n",
      "Extracted transArchive_201610.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201611.zip\n",
      "Extracted transArchive_201611.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201612.zip\n",
      "Extracted transArchive_201612.zip to data\\extracted_zips_big\n",
      "Attempting to extract: C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big\\transArchive_201701.zip\n",
      "Extracted transArchive_201701.zip to data\\extracted_zips_big\n",
      "All files extracted.\n"
     ]
    }
   ],
   "source": [
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over all the files in the directory\n",
    "for idx, filename in enumerate(os.listdir(directory_path)) : # JC: I added the enumerate so I can test on small samples.\n",
    "    if filename.endswith('.zip'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Print the file path for debugging\n",
    "        print(f\"Attempting to extract: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all the contents into the directory\n",
    "                zip_ref.extractall(output_folder) # JC: so I changed this\n",
    "                print(f\"Extracted {filename} to {output_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "        #if idx > 10:\n",
    "            #break\n",
    "print(\"All files extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn messy CSV to clean CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(clean_output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "datetime,register_no,emp_no,trans_no,upc,description,trans_type,trans_subtype,trans_status,department,quantity,Scale,cost,unitPrice,total,regPrice,altPrice,tax,taxexempt,foodstamp,wicable,discount,memDiscount,discountable,discounttype,voided,percentDiscount,ItemQtty,volDiscType,volume,VolSpecial,mixMatch,matched,memType,staff,numflag,itemstatus,tenderstatus,charflag,varflag,batchHeaderID,local,organic,display,receipt,card_no,store,branch,match_id,trans_id\n",
      "datetime,register_no,emp_no,trans_no,upc,description,trans_type,trans_subtype,trans_status,department,quantity,Scale,cost,unitPrice,total,regPrice,altPrice,tax,taxexempt,foodstamp,wicable,discount,memDiscount,discountable,discounttype,voided,percentDiscount,ItemQtty,volDiscType,volume,VolSpecial,mixMatch,matched,memType,staff,numflag,itemstatus,tenderstatus,charflag,varflag,batchHeaderID,local,organic,display,receipt,card_no,store,branch,match_id,trans_id\n",
      "datetime,register_no,emp_no,trans_no,upc,description,trans_type,trans_subtype,trans_status,department,quantity,Scale,cost,unitPrice,total,regPrice,altPrice,tax,taxexempt,foodstamp,wicable,discount,memDiscount,discountable,discounttype,voided,percentDiscount,ItemQtty,volDiscType,volume,VolSpecial,mixMatch,matched,memType,staff,numflag,itemstatus,tenderstatus,charflag,varflag,batchHeaderID,local,organic,display,receipt,card_no,store,branch,match_id,trans_id\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:30: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:27: DtypeWarning: Columns (33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\";\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"\n",
      "2015-11-01 07:21:50,51,94,4,TAX,Tax,A,,,0,0,0,0.0000,0.0000,0.4800,0.0000,\\N,0,0,0,\\N,0.0000,0.0000,0,0,0,\\N,0,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,\\N,,0,52595,1,3,0,12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-01 07:03:06,51,94,2,TAX,Tax,A,,,0,0,0,0.0000,0.0000,0.0000,0.0000,,0,0,0,,0.0000,0.0000,0,0,0,,0,0,0,0.0000,0,0,0,,0,0,0,,,,0,,,0,3,1,3,0,7\n",
      "2016-01-01 09:12:14,51,94,3,0000000000039,Wedge Scone,I, , ,8,3,0,0.5160,2.4900,7.4700,2.4900,\\N,0,0,1,\\N,0.0000,0.0000,1,0,0,\\N,3,0,0,0.0000,0,0,0,\\N,5,0,0,,\\N,\\N,0,0,,0,3,1,3,0,10\n",
      "2016-02-01 07:16:56,51,94,3,TAX,Tax,A,,,0,0,0,0.0000,0.0000,0.2700,0.0000,\\N,0,0,0,\\N,0.0000,0.0000,0,0,0,\\N,0,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,\\N,,0,3,1,3,0,10\n",
      "2016-03-01 07:04:38,51,94,2,TAX,Tax,A,,,0,0,0,0.0000,0.0000,0.0000,0.0000,\\N,0,0,0,\\N,0.0000,0.0000,0,0,0,\\N,0,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,\\N,,0,3,1,3,0,10\n",
      "2016-04-01 07:34:35,51,94,18,0,Cash,T,CA,,0,0,0,0.0000,0.0000,-5.0000,0.0000,\\N,0,0,0,\\N,0.0000,0.0000,0,0,0,\\N,0,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,\\N,,0,49019,1,3,0,9\n",
      "2016-05-01 11:23:35,51,94,113,0000000004365,BBOWL SuperSoba Chicken/Seitan,I, , ,8,1,0,0.0000,10.0000,10.0000,10.0000,\\N,1,0,0,0,0.0000,0.0000,7,0,0,0.00000000,1,0,0,0.0000,0,0,0,\\N,1,0,0,,\\N,\\N,0,0,,0,12539,1,3,0,4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-01 08:04:44,51,94,41,0000000000151,Banana Organic,I, , ,2,1.45,1,0.8900,1.1900,1.7300,1.1900,\\N,0,0,1,1,0.0000,0.0000,1,0,0,10.00000000,1.45,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,1,,0,12367,1,3,0,2\n",
      "2016-07-01 07:06:15,51,94,1,0065722700050,Electrolyte Water 1.5L Essenti,I, , ,1,3,0,1.5800,2.6900,8.0700,2.6900,\\N,0,0,1,0,0.0000,0.0000,1,0,0,0.00000000,3,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,0,,0,3,1,3,0,1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-01 07:34:16,51,94,7,0000000000151,Banana Organic,I, , ,2,0.52,1,0.8900,1.1900,0.6200,1.1900,\\N,0,0,1,1,0.0000,0.0000,1,0,0,0.00000000,0.52,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,1,,0,21998,1,3,0,2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-09-01 07:13:09,51,94,6,0,Change,T,CA,,0,0,0,0.0000,0.0000,0.0000,0.0000,\\N,0,0,0,0,0.0000,0.0000,0,0,8,\\N,0,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,\\N,,0,20074,1,3,0,7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-01 07:04:40,51,94,1,DISCOUNT,Discount,I,,,0,1,0,0.0000,0.0000,0.0000,0.0000,\\N,0,0,0,0,0.0000,0.0000,0,0,0,\\N,1,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,\\N,,0,49355,1,3,0,10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-11-01 07:18:44,51,94,11,0000000001014,Green Patch Redemption,I, , ,1,1,0,0.0000,-0.1000,-0.1000,-0.1000,\\N,0,0,0,0,0.0000,0.0000,0,0,0,10.00000000,1,0,0,0.0000,0,0,0,\\N,4,0,0,,\\N,\\N,0,-1,,0,16646,1,3,0,13\n",
      "2016-12-01 07:43:01,51,94,23,0000000000049,Wedge Muffin,I, , ,8,1,0,0.6350,2.4900,2.4900,2.4900,\\N,0,0,1,0,0.0000,0.0000,7,0,0,10.00000000,1,0,0,0.0000,0,0,0,\\N,5,0,0,,\\N,\\N,0,0,,0,13863,1,3,0,2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01 09:00:31,51,94,12,0,Change,T,CA,,0,0,0,0.0000,0.0000,0.0000,0.0000,\\N,0,0,0,0,0.0000,0.0000,0,0,8,\\N,0,0,0,0.0000,0,0,0,\\N,0,0,0,,\\N,\\N,0,\\N,,0,24528,1,3,0,12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\631833568.py:36: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n"
     ]
    }
   ],
   "source": [
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "# loop though all files in the directory\n",
    "# JC: you did this in a bizarre way. Compare to this.\n",
    "\n",
    "extracted_files = os.listdir(output_folder)\n",
    "\n",
    "for file in extracted_files : \n",
    "    # Now we pick up with yours. \n",
    "    if file.endswith('.csv'):\n",
    "        with open(output_folder + \"/\" + file,'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "\n",
    "        print(first_line)\n",
    "\n",
    "        # Handle different delimiters\n",
    "        if \"datetime\" in first_line:\n",
    "            # Check for comma as delimiter\n",
    "            if \",\" in first_line:\n",
    "                \n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\")\n",
    "        \n",
    "            elif \";\" in first_line:\n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\")\n",
    "            else:\n",
    "                print(f\"Neither , or ; in {file}\")\n",
    "        else:\n",
    "            # Add headers to the file and then read it\n",
    "            if \",\" in first_line:\n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep=\",\", header=None, names=correct_headers)\n",
    "        \n",
    "            elif \";\" in first_line:\n",
    "                messy_data = pd.read_csv(output_folder + \"/\" + file, sep= \";\", header=None, names=correct_headers)\n",
    "            else:\n",
    "                print(f\"Neither , or ; in {file}\")\n",
    "\n",
    "        messy_data.to_csv(clean_output_folder + \"/\" + file.replace(\"csv\", \"txt\"), sep=\"\\t\", index = False)\n",
    "\n",
    "        ### Work on null NULL \\\\N - replace with \"\" (this empty string)\n",
    "        messy_data.replace([\"NULL\", \"\\\\N\"], \"\", inplace=True)  ## Chat said to put this above the above line.\n",
    "        ### some files are , delimited and some product desc have , in them. PD might handle. \n",
    "\n",
    "        #if '\"' in first_line : \n",
    "            #with open(output_folder + \"/\" + file,'r') as f:\n",
    "                #content = f.read()\n",
    "                #print(content[:1000])\n",
    "                #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning files: headers, delimeters, nulls and quotes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "# loop though all files in the directory\n",
    "# JC: you did this in a bizarre way. Compare to this.\n",
    "\n",
    "extracted_files = os.listdir(output_folder)\n",
    "\n",
    "for file in extracted_files : \n",
    "    # Now we pick up with yours. \n",
    "    if file.endswith('.csv'):\n",
    "        with open(output_folder + \"/\" + file,'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "\n",
    "        print(first_line)\n",
    "\n",
    "        if '\"' in first_line : \n",
    "            with open(output_folder + \"/\" + file,'r') as f:\n",
    "                content = f.read()\n",
    "                print(content[:1000])\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "\n",
    "correct_headers = [\n",
    "    \"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\",\n",
    "    \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "    \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\",\n",
    "    \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\",\n",
    "    \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \n",
    "    \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "]\n",
    "# loop though all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        if file.endswith('.csv'):\n",
    "            with open(full_path,'r') as f:\n",
    "                first_line = f.readline().strip()\n",
    "\n",
    "            with open(full_path,'r') as f:\n",
    "                content = f.read()\n",
    "            # check if the file likely has headers based on the first line\n",
    "            if not first_line.startswith('\"datetime\"') and not first_line.startswith('datetime'):\n",
    "                content = ','.join(correct_headers) + '\\n' + content\n",
    "\n",
    "            #content = content.replace('\\\"','inch')\n",
    "\n",
    "            #with open(full_path,'w') as f:``\n",
    "                #f.write(content)\n",
    "\n",
    "            content = '\\n'.join(['inch' + line.strip('\\\"') + 'inch' for line in content.split('\\n')])\n",
    "\n",
    "            with open(full_path, 'w') as f:\n",
    "                f.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bianca's info\n",
    "#service_path = \"/Users/biancabostrom/Documents/ADA/Wedge\\ Project/wedge-404400-cb3a632effa5.json\"\n",
    "#service_file = 'wedge-404400-cb3a632effa5.json' \n",
    "#gbq_proj_id = \"wedge-404400\" \n",
    "#gbq_dataset_id = \"wedge_data\"\n",
    "#credentials = service_account.Credentials.from_service_account_file(\"/Users/biancabostrom/Documents/ADA/Wedge Project/wedge-404400-cb3a632effa5.json\")\n",
    "\n",
    "# Spencer's info\n",
    "service_path = r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\"\n",
    "service_file = 'leafy-sunrise-403222-f51fcd80b921.json' # change this to your authentication information  \n",
    "gbq_proj_id = \"leafy-sunrise-403222\" # change this to your project. \n",
    "gbq_dataset_id = \"wedge_data\"\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path)\n",
    "\n",
    "private_key = service_path + service_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    {\"name\": \"datetime\", \"type\": \"TIMESTAMP\"},     # 1\n",
    "    {\"name\": \"register_no\", \"type\": \"FLOAT\"},      # 2\n",
    "    {\"name\": \"emp_no\", \"type\": \"FLOAT\"},           # 3\n",
    "    {\"name\": \"trans_no\", \"type\": \"FLOAT\"},         # 4\n",
    "    {\"name\": \"upc\", \"type\": \"STRING\"},             # 5\n",
    "    {\"name\": \"description\", \"type\": \"STRING\"},     # 6\n",
    "    {\"name\": \"trans_type\", \"type\": \"STRING\"},      # 7\n",
    "    {\"name\": \"trans_subtype\", \"type\": \"STRING\"},   # 8\n",
    "    {\"name\": \"trans_status\", \"type\": \"STRING\"},    # 9\n",
    "    {\"name\": \"department\", \"type\": \"FLOAT\"},       # 10\n",
    "    {\"name\": \"quantity\", \"type\": \"FLOAT\"},         # 11\n",
    "    {\"name\": \"Scale\", \"type\": \"FLOAT\"},            # 12\n",
    "    {\"name\": \"cost\", \"type\": \"FLOAT\"},             # 13\n",
    "    {\"name\": \"unitPrice\", \"type\": \"FLOAT\"},        # 14\n",
    "    {\"name\": \"total\", \"type\": \"FLOAT\"},            # 15\n",
    "    {\"name\": \"regPrice\", \"type\": \"FLOAT\"},         # 16\n",
    "    {\"name\": \"altPrice\", \"type\": \"FLOAT\"},         # 17\n",
    "    {\"name\": \"tax\", \"type\": \"FLOAT\"},              # 18\n",
    "    {\"name\": \"taxexempt\", \"type\": \"FLOAT\"},        # 19\n",
    "    {\"name\": \"foodstamp\", \"type\": \"FLOAT\"},        # 20\n",
    "    {\"name\": \"wicable\", \"type\": \"FLOAT\"},          # 21\n",
    "    {\"name\": \"discount\", \"type\": \"FLOAT\"},         # 22\n",
    "    {\"name\": \"memDiscount\", \"type\": \"FLOAT\"},      # 23\n",
    "    {\"name\": \"discountable\", \"type\": \"FLOAT\"},     # 24\n",
    "    {\"name\": \"discounttype\", \"type\": \"FLOAT\"},     # 25\n",
    "    {\"name\": \"voided\", \"type\": \"FLOAT\"},           # 26\n",
    "    {\"name\": \"percentDiscount\", \"type\": \"FLOAT\"},  # 27\n",
    "    {\"name\": \"ItemQtty\", \"type\": \"FLOAT\"},         # 28\n",
    "    {\"name\": \"volDiscType\", \"type\": \"FLOAT\"},      # 29\n",
    "    {\"name\": \"volume\", \"type\": \"FLOAT\"},           # 30\n",
    "    {\"name\": \"VolSpecial\", \"type\": \"FLOAT\"},       # 31\n",
    "    {\"name\": \"mixMatch\", \"type\": \"FLOAT\"},         # 32\n",
    "    {\"name\": \"matched\", \"type\": \"FLOAT\"},          # 33\n",
    "    {\"name\": \"memType\", \"type\": \"BOOLEAN\"},        # 34\n",
    "    {\"name\": \"staff\", \"type\": \"BOOLEAN\"},          # 35\n",
    "    {\"name\": \"numflag\", \"type\": \"FLOAT\"},          # 36\n",
    "    {\"name\": \"itemstatus\", \"type\": \"FLOAT\"},       # 37\n",
    "    {\"name\": \"tenderstatus\", \"type\": \"FLOAT\"},     # 38\n",
    "    {\"name\": \"charflag\", \"type\": \"STRING\"},        # 39\n",
    "    {\"name\": \"varflag\", \"type\": \"FLOAT\"},          # 40\n",
    "    {\"name\": \"batchHeaderID\", \"type\": \"BOOLEAN\"},  # 41\n",
    "    {\"name\": \"local\", \"type\": \"FLOAT\"},            # 42\n",
    "    {\"name\": \"organic\", \"type\": \"FLOAT\"},          # 43\n",
    "    {\"name\": \"display\", \"type\": \"BOOLEAN\"},        # 44\n",
    "    {\"name\": \"receipt\", \"type\": \"FLOAT\"},          # 45\n",
    "    {\"name\": \"card_no\", \"type\": \"FLOAT\"},          # 46\n",
    "    {\"name\": \"store\", \"type\": \"FLOAT\"},            # 47\n",
    "    {\"name\": \"branch\", \"type\": \"FLOAT\"},           # 48\n",
    "    {\"name\": \"match_id\", \"type\": \"FLOAT\"},         # 49\n",
    "    {\"name\": \"trans_id\", \"type\": \"FLOAT\"}          # 50\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201001_201003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201004_201006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201007_201009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201010_201012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201101_201103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201107_201109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201110_201112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201201_201203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201201_201203_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201204_201206: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201204_201206_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201207_201209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201207_201209_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201210_201212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201210_201212_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201301_201303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201301_201303_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201304_201306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201304_201306_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201307_201309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201307_201309_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201310_201312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201310_201312_inactive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201401_201403: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201401_201403_inactive: Expected bytes, got a 'float' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201404_201406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201404_201406_inactive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201407_201409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201407_201409_inactive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201410_201412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201410_201412_inactive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201501_201503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201504_201506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201507_201509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201511: Expected bytes, got a 'int' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201605: Expected bytes, got a 'int' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201607: Expected bytes, got a 'int' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201608: Expected bytes, got a 'int' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201609: Expected bytes, got a 'int' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201610: Expected bytes, got a 'int' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery table: transArchive_201611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201612: Expected bytes, got a 'int' object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hills\\AppData\\Local\\Temp\\ipykernel_18644\\1791103265.py:8: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data to BigQuery table transArchive_201701: Expected bytes, got a 'int' object\n"
     ]
    }
   ],
   "source": [
    "# Set up BigQuery client\n",
    "client = bigquery.Client(project=gbq_proj_id, credentials=credentials)\n",
    "\n",
    "# Loop through all files in the clean output folder\n",
    "for file in os.listdir(clean_output_folder):\n",
    "    if file.endswith('.txt'):\n",
    "        # Read the cleaned data from the .txt file\n",
    "        cleaned_data = pd.read_csv(os.path.join(clean_output_folder, file), sep='\\t')\n",
    "\n",
    "        # Create a BigQuery table name using the file name\n",
    "        table_name = file.replace('.txt', '')\n",
    "\n",
    "        # Define the BigQuery schema\n",
    "        schema = [\n",
    "            {\"name\": col, \"type\": cleaned_data[col].dtype.name.lower()}\n",
    "            for col in cleaned_data.columns\n",
    "        ]\n",
    "\n",
    "        # Create the BigQuery table\n",
    "        table_ref = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "        job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "        try:\n",
    "            # Upload data to BigQuery\n",
    "            cleaned_data.to_gbq(destination_table=table_ref, project_id=gbq_proj_id, if_exists=\"replace\")\n",
    "            print(f\"Data uploaded to BigQuery table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading data to BigQuery table {table_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found TXT file: transArchive_201001_201003.txt\n",
      "Detected delimiter: None\n",
      "Table 'leafy-sunrise-403222.wedge_data.transArchive_201001_201003' not found, skipping deletion.\n",
      "Uploading transArchive_201001_201003 to BigQuery...\n"
     ]
    },
    {
     "ename": "GenericGBQException",
     "evalue": "Reason: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/leafy-sunrise-403222/datasets/wedge_data/tables?prettyPrint=false: Invalid field name \"datetime\tregister_no\temp_no\ttrans_no\tupc\tdescription\ttrans_type\ttrans_subtype\ttrans_status\tdepartment\tquantity\tscale\tcost\tunitprice\ttotal\tregprice\taltprice\ttax\ttaxexempt\tfoodstamp\twicable\tdiscount\tmemdiscount\tdiscountable\tdiscounttype\tvoided\tpercentdiscount\titemqtty\tvoldisctype\tvolume\tvolspecial\tmixmatch\tmatched\tmemtype\tstaff\tnumflag\titemstatus\ttenderstatus\tcharflag\tvarflag\tbatchheaderid\tlocal\torganic\tdisplay\treceipt\tcard_no\tstore\tbranch\tmatch_id\ttrans_id\". Fields must contain the allowed characters, and be at most 300 characters long. For allowed characters, please refer to https://cloud.google.com/bigquery/docs/schemas#column_names",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas_gbq\\gbq.py:1197\u001b[0m, in \u001b[0;36mto_gbq\u001b[1;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, api_method, verbose, private_key, auth_redirect_uri, client_id, client_secret)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1196\u001b[0m     \u001b[39m# Try to get the table\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     table \u001b[39m=\u001b[39m bqclient\u001b[39m.\u001b[39;49mget_table(destination_table_ref)\n\u001b[0;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m google_exceptions\u001b[39m.\u001b[39mNotFound:\n\u001b[0;32m   1199\u001b[0m     \u001b[39m# If the table doesn't already exist, create it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:1060\u001b[0m, in \u001b[0;36mClient.get_table\u001b[1;34m(self, table, retry, timeout)\u001b[0m\n\u001b[0;32m   1059\u001b[0m span_attributes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m: path}\n\u001b[1;32m-> 1060\u001b[0m api_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_api(\n\u001b[0;32m   1061\u001b[0m     retry,\n\u001b[0;32m   1062\u001b[0m     span_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBigQuery.getTable\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1063\u001b[0m     span_attributes\u001b[39m=\u001b[39;49mspan_attributes,\n\u001b[0;32m   1064\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1065\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m   1066\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   1067\u001b[0m )\n\u001b[0;32m   1068\u001b[0m \u001b[39mreturn\u001b[39;00m Table\u001b[39m.\u001b[39mfrom_api_repr(api_response)\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:808\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m     \u001b[39mwith\u001b[39;00m create_span(\n\u001b[0;32m    806\u001b[0m         name\u001b[39m=\u001b[39mspan_name, attributes\u001b[39m=\u001b[39mspan_attributes, client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, job_ref\u001b[39m=\u001b[39mjob_ref\n\u001b[0;32m    807\u001b[0m     ):\n\u001b[1;32m--> 808\u001b[0m         \u001b[39mreturn\u001b[39;00m call()\n\u001b[0;32m    810\u001b[0m \u001b[39mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\retry.py:366\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    365\u001b[0m )\n\u001b[1;32m--> 366\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    367\u001b[0m     target,\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    369\u001b[0m     sleep_generator,\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    371\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    372\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\retry.py:204\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m target()\n\u001b[0;32m    206\u001b[0m \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\_http\\__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m    496\u001b[0m \u001b[39mif\u001b[39;00m expect_json \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mcontent:\n",
      "\u001b[1;31mNotFound\u001b[0m: 404 GET https://bigquery.googleapis.com/bigquery/v2/projects/leafy-sunrise-403222/datasets/wedge_data/tables/transArchive_201001_201003?prettyPrint=false: Not found: Table leafy-sunrise-403222:wedge_data.transArchive_201001_201003",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas_gbq\\gbq.py:1352\u001b[0m, in \u001b[0;36m_Table.create\u001b[1;34m(self, table_id, schema)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1352\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate_table(table)\n\u001b[0;32m   1353\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhttp_error \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:774\u001b[0m, in \u001b[0;36mClient.create_table\u001b[1;34m(self, table, exists_ok, retry, timeout)\u001b[0m\n\u001b[0;32m    773\u001b[0m span_attributes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m: path, \u001b[39m\"\u001b[39m\u001b[39mdataset_id\u001b[39m\u001b[39m\"\u001b[39m: dataset_id}\n\u001b[1;32m--> 774\u001b[0m api_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_api(\n\u001b[0;32m    775\u001b[0m     retry,\n\u001b[0;32m    776\u001b[0m     span_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBigQuery.createTable\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    777\u001b[0m     span_attributes\u001b[39m=\u001b[39;49mspan_attributes,\n\u001b[0;32m    778\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    779\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    780\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    781\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    782\u001b[0m )\n\u001b[0;32m    783\u001b[0m \u001b[39mreturn\u001b[39;00m Table\u001b[39m.\u001b[39mfrom_api_repr(api_response)\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:808\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m     \u001b[39mwith\u001b[39;00m create_span(\n\u001b[0;32m    806\u001b[0m         name\u001b[39m=\u001b[39mspan_name, attributes\u001b[39m=\u001b[39mspan_attributes, client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, job_ref\u001b[39m=\u001b[39mjob_ref\n\u001b[0;32m    807\u001b[0m     ):\n\u001b[1;32m--> 808\u001b[0m         \u001b[39mreturn\u001b[39;00m call()\n\u001b[0;32m    810\u001b[0m \u001b[39mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\retry.py:366\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    365\u001b[0m )\n\u001b[1;32m--> 366\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    367\u001b[0m     target,\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    369\u001b[0m     sleep_generator,\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    371\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    372\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\retry.py:204\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m target()\n\u001b[0;32m    206\u001b[0m \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\_http\\__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m    496\u001b[0m \u001b[39mif\u001b[39;00m expect_json \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mcontent:\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/leafy-sunrise-403222/datasets/wedge_data/tables?prettyPrint=false: Invalid field name \"datetime\tregister_no\temp_no\ttrans_no\tupc\tdescription\ttrans_type\ttrans_subtype\ttrans_status\tdepartment\tquantity\tscale\tcost\tunitprice\ttotal\tregprice\taltprice\ttax\ttaxexempt\tfoodstamp\twicable\tdiscount\tmemdiscount\tdiscountable\tdiscounttype\tvoided\tpercentdiscount\titemqtty\tvoldisctype\tvolume\tvolspecial\tmixmatch\tmatched\tmemtype\tstaff\tnumflag\titemstatus\ttenderstatus\tcharflag\tvarflag\tbatchheaderid\tlocal\torganic\tdisplay\treceipt\tcard_no\tstore\tbranch\tmatch_id\ttrans_id\". Fields must contain the allowed characters, and be at most 300 characters long. For allowed characters, please refer to https://cloud.google.com/bigquery/docs/schemas#column_names",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\ILOVEPART1.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X42sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [col\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X42sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUploading \u001b[39m\u001b[39m{\u001b[39;00mtable_name\u001b[39m}\u001b[39;00m\u001b[39m to BigQuery...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X42sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m pandas_gbq\u001b[39m.\u001b[39;49mto_gbq(df, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mgbq_dataset_id\u001b[39m}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m{\u001b[39;49;00mtable_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, project_id\u001b[39m=\u001b[39;49mgbq_proj_id, if_exists\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mreplace\u001b[39;49m\u001b[39m'\u001b[39;49m, credentials\u001b[39m=\u001b[39;49mcredentials, table_schema\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hills/Documents/Fall2023/ADA/wedge-project/ILOVEPART1.ipynb#X42sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mdel\u001b[39;00m df  \u001b[39m# Clean the DataFrame from memory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas_gbq\\gbq.py:1206\u001b[0m, in \u001b[0;36mto_gbq\u001b[1;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, api_method, verbose, private_key, auth_redirect_uri, client_id, client_secret)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m google_exceptions\u001b[39m.\u001b[39mNotFound:\n\u001b[0;32m   1199\u001b[0m     \u001b[39m# If the table doesn't already exist, create it\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m     table_connector \u001b[39m=\u001b[39m _Table(\n\u001b[0;32m   1201\u001b[0m         project_id_table,\n\u001b[0;32m   1202\u001b[0m         dataset_id,\n\u001b[0;32m   1203\u001b[0m         location\u001b[39m=\u001b[39mlocation,\n\u001b[0;32m   1204\u001b[0m         credentials\u001b[39m=\u001b[39mconnector\u001b[39m.\u001b[39mcredentials,\n\u001b[0;32m   1205\u001b[0m     )\n\u001b[1;32m-> 1206\u001b[0m     table_connector\u001b[39m.\u001b[39;49mcreate(table_id, table_schema)\n\u001b[0;32m   1207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1208\u001b[0m     \u001b[39m# Convert original schema (the schema that already exists) to pandas-gbq API format\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m     original_schema \u001b[39m=\u001b[39m pandas_gbq\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mto_pandas_gbq(table\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas_gbq\\gbq.py:1354\u001b[0m, in \u001b[0;36m_Table.create\u001b[1;34m(self, table_id, schema)\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate_table(table)\n\u001b[0;32m   1353\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhttp_error \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m-> 1354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_http_error(ex)\n",
      "File \u001b[1;32mc:\\Users\\hills\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas_gbq\\gbq.py:396\u001b[0m, in \u001b[0;36mGbqConnector.process_http_error\u001b[1;34m(ex)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[39mraise\u001b[39;00m TableCreationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReason: \u001b[39m\u001b[39m{\u001b[39;00merror_message\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    395\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 396\u001b[0m     \u001b[39mraise\u001b[39;00m GenericGBQException(\u001b[39m\"\u001b[39m\u001b[39mReason: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(ex)) \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n",
      "\u001b[1;31mGenericGBQException\u001b[0m: Reason: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/leafy-sunrise-403222/datasets/wedge_data/tables?prettyPrint=false: Invalid field name \"datetime\tregister_no\temp_no\ttrans_no\tupc\tdescription\ttrans_type\ttrans_subtype\ttrans_status\tdepartment\tquantity\tscale\tcost\tunitprice\ttotal\tregprice\taltprice\ttax\ttaxexempt\tfoodstamp\twicable\tdiscount\tmemdiscount\tdiscountable\tdiscounttype\tvoided\tpercentdiscount\titemqtty\tvoldisctype\tvolume\tvolspecial\tmixmatch\tmatched\tmemtype\tstaff\tnumflag\titemstatus\ttenderstatus\tcharflag\tvarflag\tbatchheaderid\tlocal\torganic\tdisplay\treceipt\tcard_no\tstore\tbranch\tmatch_id\ttrans_id\". Fields must contain the allowed characters, and be at most 300 characters long. For allowed characters, please refer to https://cloud.google.com/bigquery/docs/schemas#column_names"
     ]
    }
   ],
   "source": [
    "# Move through all files in the directory\n",
    "for root, dirs, files in os.walk(clean_output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "\n",
    "        if file.endswith('.txt'):\n",
    "            print(f\"Found TXT file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "\n",
    "            # Reading TXT with correct handling of quoted fields\n",
    "            df = pd.read_csv(full_path, delimiter=delimiter, quotechar='\"', dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('.txt', '')\n",
    "\n",
    "            # Drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "\n",
    "            # Construct the fully-qualified table_id without \".txt\" extension\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"Deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"Table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            # Explicitly convert columns to the correct data types\n",
    "            # Adjust these conversions based on your actual column names and data types\n",
    "            #df['column_name_1'] = pd.to_numeric(df['column_name_1'], errors='coerce')\n",
    "            #df['column_name_2'] = pd.to_numeric(df['column_name_2'], errors='coerce')\n",
    "            # Repeat the above line for any columns causing the conversion error\n",
    "\n",
    "            # Clean the DataFrame\n",
    "            df = clean_dataframe(df)\n",
    "\n",
    "            # Modify the field names to comply with the gbq rules\n",
    "            df.columns = [col.lower().replace(';', '') for col in df.columns]\n",
    "\n",
    "            print(f\"Uploading {table_name} to BigQuery...\")\n",
    "            pandas_gbq.to_gbq(df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "            del df  # Clean the DataFrame from memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi John - cells below are for reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code chunk\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "def drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "    \n",
    "def clean_dataframe(df):\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice'\n",
    "        , 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype'\n",
    "        , 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag'\n",
    "        , 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'receipt', 'card_no', 'store', 'branch', 'match_id'\n",
    "        ,'trans_id'\n",
    "    ]\n",
    "\n",
    "    boolean_columns = [ 'memType', 'staff', 'batchHeaderID', 'display']\n",
    "\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col]  = df[col].astype(str)\n",
    "            df[col] = df[col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df[float_columns] = df[float_columns].fillna(0)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    replace_strings = [\"\\\\n\", \"\\\\\\\\\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    for col in ['itemQtty', 'reciept']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace('\"', '', regex=False)\n",
    "\n",
    "    # Print the first 20 rows for inspection\n",
    "    print(\"First 20 rows after cleaning:\")\n",
    "    print(df.head(20))\n",
    "\n",
    "    df = df.applymap(lambda x: None if x == '' else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hold on to this while I test another chunk - more than likley delete\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "def drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "    \n",
    "def clean_dataframe(df):\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice'\n",
    "        , 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype'\n",
    "        , 'voided', 'percentDiscount', 'itemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag'\n",
    "        , 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'reciept', 'card_no', 'store', 'branch', 'match_id'\n",
    "        ,'trans_id'\n",
    "    ]\n",
    "\n",
    "    boolean_columns = [ 'memType', 'staff', 'batchHeaderID', 'display']\n",
    "\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "        # Check if columns exist before applying operations\n",
    "\n",
    "    # List of columns to check and clean\n",
    "    #columns_to_clean = ['itemQtty', 'reciept']\n",
    "\n",
    "   # for col in columns_to_clean:\n",
    "    #    if col in df.columns:\n",
    "     #       if col == 'charflag':\n",
    "      #          df[col] = df[col].str.strip()\n",
    "       #     elif col in ['itemQtty', 'reciept']:\n",
    "        #        df[col] = df[col].astype(str).str.replace('\"', '', regex=False)\n",
    "\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col]  = df[col].astype(str)\n",
    "            df [col] = df [col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df[float_columns] = df[float_columns].fillna(0)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    \n",
    "    replace_strings = [\"\\\\n\", \"\\\\\\\\\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "            #df[col] = df[col].str.replace('\\\\\\\"', '', regex=False)\n",
    "\n",
    "    for col in df.columns: # added these three lines trying to problem solve next chuck for gbq file path\n",
    "        if col == 'charflag' and col in df.columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    df = df.applymap(lambda x: None if x == '' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#duplicate code to play with\n",
    "\n",
    "# move through all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"detected delimiter: {delimiter}\")\n",
    "\n",
    "            # reading csv with correct handling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('.csv', '')\n",
    "\n",
    "            # drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    \n",
    "            # Construct the fully-qualified table_id without \".csv\" extension\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            print(f\"reading csv file in chunks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # clean the DF\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                # modify the field names to comply with the gbq rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # for the first chunk create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # for subsequent chunks, append to the table \n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # clean the chunk from memory\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move through all files in the directory\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"detected delimiter: {delimiter}\")\n",
    "\n",
    "            #reading csv with correct handeling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter,quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('data.csv', '')\n",
    "\n",
    "            #drop the table if it exists\n",
    "            drop_table_if_exists(gbq_dataset_id, table_name, credentials, gbq_proj_id)\n",
    "            client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    \n",
    "                # Construct the fully-qualified table_id\n",
    "            table_id = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_name}\"\n",
    "\n",
    "            try:\n",
    "                client.delete_table(table_id)\n",
    "                print(f\"deleted table '{table_id}'\")\n",
    "            except NotFound:\n",
    "                print(f\"table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "            print(f\"reading csv file in chucks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                #clean the DF\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                #modify the field names to comply with the gbq rules\n",
    "                chunk_df.columns = [col.lower().replace(';','') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # for the first chuck create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # for subsuquent chunks, append to the table \n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{gbq_dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df # clean the chunk from memory\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
