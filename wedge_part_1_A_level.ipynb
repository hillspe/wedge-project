{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from zipfile import ZipFile\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_folder = r'C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big'\n",
    "output_folder = 'delimeted_wedge_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv files have been proceessed and saved back to their respective zip files\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "      os.makedirs(output_folder)\n",
    "\n",
    "for zip_file_name in os.listdir(zip_folder):\n",
    "    if zip_file_name.endswith('.zip'):\n",
    "         zip_file_path = os.path.join(zip_folder, zip_file_name)\n",
    "\n",
    "         output_zip_file_path = os.path.join(output_folder, zip_file_name)\n",
    "         with ZipFile(output_zip_file_path, 'w') as output_zip:\n",
    "\n",
    "            with ZipFile(zip_file_path, 'r') as zf:\n",
    "                for file_name in zf.namelist():\n",
    "                    if file_name.endswith('.csv'):\n",
    "                        csv_file_name = file_name\n",
    "                        with zf.open(csv_file_name, 'r') as input_file:\n",
    "                            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "                            output_file_name = os.path.splitext(csv_file_name)[0] + '_delimited.csv'\n",
    "                            #print(output_zip_file_path)\n",
    "                            \n",
    "                            output_zip.writestr(output_file_name, '\\n'.join(','.join(line.split(';')) for line in input_file))\n",
    "                            #above will only work on ; delimted files\n",
    "                \n",
    "\n",
    "print(\"csv files have been proceessed and saved back to their respective zip files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip-files = os.listdir(\"WedgeZipOfZips\")\n",
    "#zip_folder = \"no_headers\" - was not commented out in source code\n",
    "#zip_files = os.listdir(\"text\")\n",
    "\n",
    "zip_files = [file for file in os.listdir(zip_folder) if file.endswith(\".zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hills\\\\Documents\\\\Fall2023\\\\ADA\\\\wedge-project'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    \"datetime\": \"TIMESTAMP\",  # 1\n",
    "    \"register_no\": \"FLOAT\",   # 2\n",
    "    \"emp_no\": \"FLOAT\",        # 3\n",
    "    \"trans_no\": \"FLOAT\",      # 4\n",
    "    \"upc\": \"STRING\",          # 5\n",
    "    \"description\": \"STRING\",  # 6\n",
    "    \"trans_type\": \"STRING\",   # 7\n",
    "    \"trans_subtype\": \"STRING\", # 8\n",
    "    \"trans_status\": \"STRING\",  # 9\n",
    "    \"department\": \"FLOAT\",     # 10\n",
    "    \"quantity\": \"FLOAT\",       # 11\n",
    "    \"Scale\": \"FLOAT\",          # 12\n",
    "    \"cost\": \"FLOAT\",           # 13\n",
    "    \"unitPrice\": \"FLOAT\",      # 14\n",
    "    \"total\": \"FLOAT\",          # 15\n",
    "    \"regPrice\": \"FLOAT\",       # 16\n",
    "    \"altPrice\": \"FLOAT\",       # 17\n",
    "    \"tax\": \"FLOAT\",            # 18\n",
    "    \"taxexempt\": \"FLOAT\",      # 19\n",
    "    \"foodstamp\": \"FLOAT\",      # 20\n",
    "    \"wicable\": \"FLOAT\",        # 21\n",
    "    \"discount\": \"FLOAT\",       # 22\n",
    "    \"memDiscount\": \"FLOAT\",    # 23\n",
    "    \"discountable\": \"FLOAT\",   # 24\n",
    "    \"discounttype\": \"FLOAT\",   # 25\n",
    "    \"voided\": \"FLOAT\",         # 26\n",
    "    \"percentDiscount\": \"FLOAT\",# 27\n",
    "    \"ItemQtty\": \"FLOAT\",       # 28\n",
    "    \"volDiscType\": \"FLOAT\",    # 29\n",
    "    \"volume\": \"FLOAT\",         # 30\n",
    "    \"VolSpecial\": \"FLOAT\",     # 31\n",
    "    \"mixMatch\": \"FLOAT\",       # 32\n",
    "    \"matched\": \"FLOAT\",        # 33\n",
    "    \"memType\": \"BOOLEAN\",      # 34\n",
    "    \"staff\": \"BOOLEAN\",        # 35\n",
    "    \"numflag\": \"FLOAT\",        # 36\n",
    "    \"itemstatus\": \"FLOAT\",     # 37\n",
    "    \"tenderstatus\": \"FLOAT\",   # 38\n",
    "    \"charflag\": \"STRING\",      # 39\n",
    "    \"varflag\": \"FLOAT\",        # 40\n",
    "    \"batchHeaderID\": \"BOOLEAN\", # 41\n",
    "    \"local\": \"FLOAT\",          # 42\n",
    "    \"organic\": \"FLOAT\",        # 43\n",
    "    \"display\": \"BOOLEAN\",      # 44\n",
    "    \"receipt\": \"FLOAT\",        # 45\n",
    "    \"card_no\": \"FLOAT\",        # 46\n",
    "    \"store\": \"FLOAT\",          # 47\n",
    "    \"branch\": \"FLOAT\",         # 48\n",
    "    \"match_id\": \"FLOAT\",       # 49\n",
    "    \"trans_id\": \"FLOAT\"        # 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n",
      "file: [file_name], has header: [is_header])\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"with_headers\"\n",
    "os.makedirs(output_folder, exist_ok = True)\n",
    "\n",
    "headers = dict()\n",
    "\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(os.path.join(r'C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\data\\WedgeZipOfZips_Big', this_zf), 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file = zf.open(file_name, 'r')\n",
    "            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "            first_line = input_file.readline()\n",
    "\n",
    "            is_header = any(keyword in first_line for keyword in column_types.keys())\n",
    "\n",
    "            headers[file_name] = is_header\n",
    "\n",
    "            if not is_header:\n",
    "                header_row = ','.join(column_types.keys()) + '\\n'\n",
    "                input_file.seek(0)\n",
    "                file_contents = input_file.read()\n",
    "                input_file.close()\n",
    "\n",
    "                subfolder_path = os.path.join(output_folder, os.path.splitext(file_name)[0])\n",
    "                os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "\n",
    "                output_file_path = os.path.join(subfolder_path, file_name)\n",
    "                with open(output_file_path, 'w', encoding=\"utf-8\") as output_file:\n",
    "                    output_file.write(header_row + file_contents)\n",
    "\n",
    "                print(f\"file: [file_name], has header: [is_header])\")\n",
    "\n",
    "for foldername, subfolders, filenames in os.walk(output_folder):\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(output_folder, subfolder)\n",
    "        zip_file_path = os.path.join(output_folder, subfolder + \".zip\")\n",
    "        with ZipFile(zip_file_path, 'w') as new_zip:\n",
    "            for root, dirs, files in os.walk(subfolder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    new_zip.write(file_path, os.path.relpath(file_path, subfolder_path))\n",
    "\n",
    "\n",
    "for subfolder in os.listdir(output_folder):\n",
    "    subfolder_path = os.path.join(output_folder, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            file_path = os.path.join(subfolder_path, file)\n",
    "            os.remove(file_path)\n",
    "        os.rmdir(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_path = r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\"\n",
    "service_file = 'leafy-sunrise-403222-f51fcd80b921.json' # change this to your authentication information  \n",
    "\n",
    "private_key =service_path + service_file\n",
    "credentials = service_account.Credentials.from_service_account_file(r\"C:\\Users\\hills\\Documents\\Fall2023\\ADA\\wedge-project\\leafy-sunrise-403222-f51fcd80b921.json\")\n",
    "\n",
    "gbq_proj_id = \"leafy-sunrise-403222\" # change this to your project. \n",
    "gbq_dataset_id = \"wedge_data\"\n",
    "\n",
    "# And create a client to talk to GBQ\n",
    "client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "\n",
    "zip_dir = 'wedge_zips'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    {\"name\": \"datetime\", \"type\": \"TIMESTAMP\"},     # 1\n",
    "    {\"name\": \"register_no\", \"type\": \"FLOAT\"},      # 2\n",
    "    {\"name\": \"emp_no\", \"type\": \"FLOAT\"},           # 3\n",
    "    {\"name\": \"trans_no\", \"type\": \"FLOAT\"},         # 4\n",
    "    {\"name\": \"upc\", \"type\": \"STRING\"},             # 5\n",
    "    {\"name\": \"description\", \"type\": \"STRING\"},     # 6\n",
    "    {\"name\": \"trans_type\", \"type\": \"STRING\"},      # 7\n",
    "    {\"name\": \"trans_subtype\", \"type\": \"STRING\"},   # 8\n",
    "    {\"name\": \"trans_status\", \"type\": \"STRING\"},    # 9\n",
    "    {\"name\": \"department\", \"type\": \"FLOAT\"},       # 10\n",
    "    {\"name\": \"quantity\", \"type\": \"FLOAT\"},         # 11\n",
    "    {\"name\": \"Scale\", \"type\": \"FLOAT\"},            # 12\n",
    "    {\"name\": \"cost\", \"type\": \"FLOAT\"},             # 13\n",
    "    {\"name\": \"unitPrice\", \"type\": \"FLOAT\"},        # 14\n",
    "    {\"name\": \"total\", \"type\": \"FLOAT\"},            # 15\n",
    "    {\"name\": \"regPrice\", \"type\": \"FLOAT\"},         # 16\n",
    "    {\"name\": \"altPrice\", \"type\": \"FLOAT\"},         # 17\n",
    "    {\"name\": \"tax\", \"type\": \"FLOAT\"},              # 18\n",
    "    {\"name\": \"taxexempt\", \"type\": \"FLOAT\"},        # 19\n",
    "    {\"name\": \"foodstamp\", \"type\": \"FLOAT\"},        # 20\n",
    "    {\"name\": \"wicable\", \"type\": \"FLOAT\"},          # 21\n",
    "    {\"name\": \"discount\", \"type\": \"FLOAT\"},         # 22\n",
    "    {\"name\": \"memDiscount\", \"type\": \"FLOAT\"},      # 23\n",
    "    {\"name\": \"discountable\", \"type\": \"FLOAT\"},     # 24\n",
    "    {\"name\": \"discounttype\", \"type\": \"FLOAT\"},     # 25\n",
    "    {\"name\": \"voided\", \"type\": \"FLOAT\"},           # 26\n",
    "    {\"name\": \"percentDiscount\", \"type\": \"FLOAT\"},  # 27\n",
    "    {\"name\": \"ItemQtty\", \"type\": \"FLOAT\"},         # 28\n",
    "    {\"name\": \"volDiscType\", \"type\": \"FLOAT\"},      # 29\n",
    "    {\"name\": \"volume\", \"type\": \"FLOAT\"},           # 30\n",
    "    {\"name\": \"VolSpecial\", \"type\": \"FLOAT\"},       # 31\n",
    "    {\"name\": \"mixMatch\", \"type\": \"FLOAT\"},         # 32\n",
    "    {\"name\": \"matched\", \"type\": \"FLOAT\"},          # 33\n",
    "    {\"name\": \"memType\", \"type\": \"BOOLEAN\"},        # 34\n",
    "    {\"name\": \"staff\", \"type\": \"BOOLEAN\"},          # 35\n",
    "    {\"name\": \"numflag\", \"type\": \"FLOAT\"},          # 36\n",
    "    {\"name\": \"itemstatus\", \"type\": \"FLOAT\"},       # 37\n",
    "    {\"name\": \"tenderstatus\", \"type\": \"FLOAT\"},     # 38\n",
    "    {\"name\": \"charflag\", \"type\": \"STRING\"},        # 39\n",
    "    {\"name\": \"varflag\", \"type\": \"FLOAT\"},          # 40\n",
    "    {\"name\": \"batchHeaderID\", \"type\": \"BOOLEAN\"},  # 41\n",
    "    {\"name\": \"local\", \"type\": \"FLOAT\"},            # 42\n",
    "    {\"name\": \"organic\", \"type\": \"FLOAT\"},          # 43\n",
    "    {\"name\": \"display\", \"type\": \"BOOLEAN\"},        # 44\n",
    "    {\"name\": \"receipt\", \"type\": \"FLOAT\"},          # 45\n",
    "    {\"name\": \"card_no\", \"type\": \"FLOAT\"},          # 46\n",
    "    {\"name\": \"store\", \"type\": \"FLOAT\"},            # 47\n",
    "    {\"name\": \"branch\", \"type\": \"FLOAT\"},           # 48\n",
    "    {\"name\": \"match_id\", \"type\": \"FLOAT\"},         # 49\n",
    "    {\"name\": \"trans_id\", \"type\": \"FLOAT\"}          # 50\n",
    "]\n",
    "\n",
    "date_columns = [\"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'with_headers'\n",
    "\n",
    "csv_files = [file for file in os.listdir (folder_path) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files extracted and saved in the 'wedge_extracted' folder.\n"
     ]
    }
   ],
   "source": [
    "output_folder = 'wedge_extracted'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        zip_file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "            # Get a list of extracted files\n",
    "            extracted_files = zip_file.namelist()\n",
    "            \n",
    "            # Extract all files\n",
    "            zip_file.extractall(output_folder)\n",
    "\n",
    "            if len(extracted_files) == 1 and extracted_files[0].endswith('.csv'):\n",
    "                csv_file_path = os.path.join(output_folder, extracted_files[0])\n",
    "                new_csv_name = os.path.splitext(filename)[0] + '.csv'\n",
    "                os.rename(csv_file_path, os.path.join(output_folder, new_csv_name))\n",
    "\n",
    "print(\"CSV files extracted and saved in the 'wedge_extracted' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'wedge_extracted'\n",
    "csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "custom_na_values = [r\"\\\\N\", r\"\\N\", \",\", \", \", \"nan\"] # look over the ,'s \n",
    " \n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    table_id = os.path.basename(file).split('.')[0]\n",
    "\n",
    "    table_ref = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_id}\"\n",
    "\n",
    "    current_df = pd.read_csv(file, parse_dates=date_columns, na_values=custom_na_values, keep_default_na=True)\n",
    "\n",
    "\n",
    "    column_types = {\n",
    "    \"datetime\": \"TIMESTAMP\",  # 1\n",
    "    \"register_no\": \"float64\",   # 2\n",
    "    \"emp_no\": \"float64\",        # 3\n",
    "    \"trans_no\": \"float64\",      # 4\n",
    "    \"upc\": \"str\",          # 5\n",
    "    \"description\": \"str\",  # 6\n",
    "    \"trans_type\": \"str\",   # 7\n",
    "    \"trans_subtype\": \"str\", # 8\n",
    "    \"trans_status\": \"str\",  # 9\n",
    "    \"department\": \"float64\",     # 10\n",
    "    \"quantity\": \"float64\",       # 11\n",
    "    \"Scale\": \"float64\",          # 12\n",
    "    \"cost\": \"float64\",           # 13\n",
    "    \"unitPrice\": \"float64\",      # 14\n",
    "    \"total\": \"float64\",          # 15\n",
    "    \"regPrice\": \"float64\",       # 16\n",
    "    \"altPrice\": \"float64\",       # 17\n",
    "    \"tax\": \"float64\",            # 18\n",
    "    \"taxexempt\": \"float64\",      # 19\n",
    "    \"foodstamp\": \"float64\",      # 20\n",
    "    \"wicable\": \"float64\",        # 21\n",
    "    \"discount\": \"float64\",       # 22\n",
    "    \"memDiscount\": \"float64\",    # 23\n",
    "    \"discountable\": \"float64\",   # 24\n",
    "    \"discounttype\": \"float64\",   # 25\n",
    "    \"voided\": \"float64\",         # 26\n",
    "    \"percentDiscount\": \"float64\",# 27\n",
    "    \"ItemQtty\": \"float64\",       # 28\n",
    "    \"volDiscType\": \"float64\",    # 29\n",
    "    \"volume\": \"float64\",         # 30\n",
    "    \"VolSpecial\": \"float64\",     # 31\n",
    "    \"mixMatch\": \"float64\",       # 32\n",
    "    \"matched\": \"float64\",        # 33\n",
    "    \"memType\": \"bool\",      # 34\n",
    "    \"staff\": \"bool\",        # 35\n",
    "    \"numflag\": \"float64\",        # 36\n",
    "    \"itemstatus\": \"float64\",     # 37\n",
    "    \"tenderstatus\": \"float64\",   # 38\n",
    "    \"charflag\": \"str\",      # 39\n",
    "    \"varflag\": \"float64\",        # 40\n",
    "    \"batchHeaderID\": \"bool\", # 41\n",
    "    \"local\": \"float64\",          # 42\n",
    "    \"organic\": \"float64\",        # 43\n",
    "    \"display\": \"bool\",      # 44\n",
    "    \"receipt\": \"float64\",        # 45\n",
    "    \"card_no\": \"float64\",        # 46\n",
    "    \"store\": \"float64\",          # 47\n",
    "    \"branch\": \"float64\",         # 48\n",
    "    \"match_id\": \"float64\",       # 49\n",
    "    \"trans_id\": \"float64\"        # 50\n",
    "}\n",
    "\n",
    "    for column, data_type in column_types.items():\n",
    "        if data_type == \"TIMESTAMP\":\n",
    "            current_df[column] = pd.to_datetime(current_df[column], errors='coerce')\n",
    "        else:\n",
    "            current_df[column] = current_df[column].astype(data_type)\n",
    "\n",
    "\n",
    "    # Append the current DataFrame to the main DataFrame\n",
    "    df = pd.concat([df, current_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 990.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table leafy-sunrise-403222.wedge_data.transArchive_201701 exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Upload DataFrame to BigQuery\n",
    "table_name = f\"{gbq_proj_id}.{gbq_dataset_id}.{table_id}\"\n",
    "pandas_gbq.to_gbq(df, destination_table=table_name, project_id=gbq_proj_id, if_exists='replace')\n",
    "\n",
    "# Check if the table exists\n",
    "dataset_ref = client.dataset(gbq_dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "table_exists = False\n",
    "retry = None  # Set retry to None if you don't have a specific retry configuration\n",
    "try:\n",
    "    # Attempt to get the table; if it doesn't exist, NotFound exception is raised\n",
    "    client.get_table(table_ref)\n",
    "    table_exists = True\n",
    "except NotFound:\n",
    "    pass  # Table doesn't exist\n",
    "\n",
    "# Display the result\n",
    "if not table_exists:\n",
    "    print(f\"The table {table_ref} does not exist.\")\n",
    "else:\n",
    "    print(f\"The table {table_ref} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
